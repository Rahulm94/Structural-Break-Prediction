{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvWIItAe-0fN"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/crunchdao/quickstarters/blob/master/competitions/structural-break/quickstarters/baseline/baseline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNUXnJa_-0fO"
   },
   "source": [
    "![Banner](https://raw.githubusercontent.com/crunchdao/quickstarters/refs/heads/master/competitions/structural-break/assets/banner.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lurIF1Ve-0fP"
   },
   "source": [
    "# ADIA Lab Structural Break Challenge\n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n",
    "\n",
    "\n",
    "## Challenge Overview\n",
    "\n",
    "Welcome to the ADIA Lab Structural Break Challenge! In this challenge, you will analyze univariate time series data to determine whether a structural break has occurred at a specified boundary point.\n",
    "\n",
    "### What is a Structural Break?\n",
    "\n",
    "A structural break occurs when the process governing the data generation changes at a certain point in time. These changes can be subtle or dramatic, and detecting them accurately is crucial across various domains such as climatology, industrial monitoring, finance, and healthcare.\n",
    "\n",
    "![Structural Break Example](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/structural-break/quickstarters/baseline/images/example.png)\n",
    "\n",
    "### Your Task\n",
    "\n",
    "For each time series in the test set, you need to predict a score between `0` and `1`:\n",
    "- Values closer to `0` indicate no structural break at the specified boundary point;\n",
    "- Values closer to `1` indicate a structural break did occur.\n",
    "\n",
    "### Evaluation Metric\n",
    "\n",
    "The evaluation metric is [ROC AUC (Area Under the Receiver Operating Characteristic Curve)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), which measures the performance of detection algorithms regardless of their specific calibration.\n",
    "\n",
    "- ROC AUC around `0.5`: No better than random chance;\n",
    "- ROC AUC approaching `1.0`: Perfect detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5Lvv2Pps-fT"
   },
   "source": [
    "# Setup\n",
    "\n",
    "The first steps to get started are:\n",
    "1. Get the setup command\n",
    "2. Execute it in the cell below\n",
    "\n",
    "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
    "\n",
    "![Reveal token](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/reveal-token.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DUeixiC_IJM"
   },
   "outputs": [],
   "source": [
    "%pip install crunch-cli --upgrade --quiet --progress-bar off\n",
    "!crunch setup-notebook structural-break 60q3djbznC8owz0AHJCfqGov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IBhw7hv-0fQ"
   },
   "source": [
    "# Your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpLeMWSw-0fQ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKqz-6Zw-0fR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "import tqdm\n",
    "import logging\n",
    "\n",
    "# Import your dependencies\n",
    "import joblib\n",
    "import numpy as np # == 1.26.4\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from scipy.stats import ttest_ind, ks_2samp, levene\n",
    "from scipy.stats import entropy, normaltest, jarque_bera\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.signal import periodogram\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics\n",
    "from xgboost import XGBClassifier\n",
    "from prophet import Prophet\n",
    "# from pmdarima import auto_arima # == 2.0.4\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pI3_XRw8kaV"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# !pip install numpy==1.26.4\n",
    "# !pip install pmdarima==2.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjD_WSAS-0fR"
   },
   "outputs": [],
   "source": [
    "import crunch\n",
    "\n",
    "# Load the Crunch Toolings\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiKJODFx-0fR"
   },
   "source": [
    "## Understanding the Data\n",
    "\n",
    "The dataset consists of univariate time series, each containing ~2,000-5,000 values with a designated boundary point. For each time series, you need to determine whether a structural break occurred at this boundary point.\n",
    "\n",
    "The data was downloaded when you setup your local environment and is now available in the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKHXgvjN-0fS"
   },
   "outputs": [],
   "source": [
    "# Load the data simply\n",
    "X_train, y_train, X_test = crunch.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T_JmgMq-0fS"
   },
   "source": [
    "### Understanding `X_train`\n",
    "\n",
    "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
    "\n",
    "**Index Levels:**\n",
    "- `id`: Identifies the unique time series\n",
    "- `time`: The timestep within each time series\n",
    "\n",
    "**Columns:**\n",
    "- `value`: The actual time series value at each timestep\n",
    "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oRCTnOb-0fS"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WP39dgx-0fS"
   },
   "source": [
    "### Understanding `y_train`\n",
    "\n",
    "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
    "\n",
    "**Index:**\n",
    "- `id`: the ID of the dataset\n",
    "\n",
    "**Value:**\n",
    "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPsQPdIj-0fT"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oSS08Ks-0fT"
   },
   "source": [
    "### Understanding `X_test`\n",
    "\n",
    "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
    "\n",
    "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ErbKAs--0fT"
   },
   "outputs": [],
   "source": [
    "print(\"Number of datasets:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_dTYXms-0fT"
   },
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgulFOGX-0fT"
   },
   "source": [
    "## Strategy Implementation\n",
    "\n",
    "There are multiple approaches you can take to detect structural breaks:\n",
    "\n",
    "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
    "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
    "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
    "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
    "\n",
    "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLfYIXlz-0fT"
   },
   "source": [
    "### The `train()` Function\n",
    "\n",
    "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
    "\n",
    "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOjMIc7bgX-S"
   },
   "outputs": [],
   "source": [
    "# Define helper functions for feature generation\n",
    "def extract_statistical_test_features(time_series_data, n):\n",
    "    period_0 = time_series_data[time_series_data['period'] == 0]['value']\n",
    "    period_1 = time_series_data[time_series_data['period'] == 1]['value']\n",
    "    _ , p_value_1 = ks_2samp(period_0, period_1)\n",
    "    _ , p_value_2 = ttest_ind(period_0, period_1)\n",
    "    _ , p_value_3 = levene(period_0, period_1)\n",
    "    p_values_stat_tests = pd.DataFrame({f'pval_ks_2samp_{n}': p_value_1, f'pval_ttest_ind_{n}': p_value_2, f'pval_levene_{n}': p_value_3}, index=[0])\n",
    "    return p_values_stat_tests\n",
    "\n",
    "def extract_frequency_features(time_series_data, n, sampling_rate=1, num_main_freqs=3):\n",
    "    \"\"\"\n",
    "    Extracts the main frequencies from time series data using a periodogram.\n",
    "\n",
    "    Args:\n",
    "      time_series_data (np.array): The input time series data.\n",
    "      sampling_rate (float): The sampling rate of the time series data (samples per unit time).\n",
    "      num_main_freqs (int): The number of main frequencies to extract.\n",
    "\n",
    "    Returns:\n",
    "      dict: A dictionary containing the top frequencies, their corresponding periods,\n",
    "            and their power spectral densities.\n",
    "    \"\"\"\n",
    "    # Estimate power spectral density using a periodogram\n",
    "    frequencies, power_spectral_density = periodogram(time_series_data, fs=sampling_rate)\n",
    "\n",
    "    # Get indices for the highest power spectral density values\n",
    "    top_freq_indices = np.argsort(power_spectral_density)[::-1][:num_main_freqs]\n",
    "\n",
    "    # Extract the top frequencies, powers, and calculate periods\n",
    "    main_frequencies = frequencies[top_freq_indices]\n",
    "    main_powers = power_spectral_density[top_freq_indices]\n",
    "    main_periods = 1 / main_frequencies\n",
    "\n",
    "    results = {}\n",
    "    for i in range(num_main_freqs):\n",
    "        results[f'value_freq_{i+1}_w{n}'] = main_frequencies[i]\n",
    "        results[f'value_period_{i+1}_w{n}'] = main_periods[i]\n",
    "        results[f'value_power_{i+1}_w{n}'] = main_powers[i]\n",
    "\n",
    "    df_freq = pd.DataFrame(results, index=[0])\n",
    "    return df_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYu5aXF7HIle"
   },
   "outputs": [],
   "source": [
    "# Generate features from values before and after breakpoint\n",
    "def generate_features(df: pd.DataFrame):\n",
    "    # Define number of windows\n",
    "    n_points = df.groupby('period')['id'].count().min()\n",
    "    start = [s for s in range(0, 50, 10)]\n",
    "    windows = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90] + [w for w in range(100, 1001, 50)]\n",
    "    print('STARTED FEATURE GENERATION: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "\n",
    "    # Define aggregations functions for pre & post-breakpoint data\n",
    "    agg_funcs = [('mean', 'mean'), ('std', 'std'), ('max', 'max'), ('min', 'min'), ('median', 'median'), ('skew', 'skew'), ('kurtosis', lambda y: y.kurt()),\n",
    "                 ('ptp', lambda y: np.ptp(y)), ('percentile_25', lambda y: np.percentile(y, 25)), ('percentile_75', lambda y: np.percentile(y, 75)),\n",
    "                 ('count', 'count'), ('first', 'first'), ('last', 'last'),\n",
    "                 ('trend', lambda y: np.mean(np.diff(y))), ('volatility', lambda y: np.std(np.diff(y))), ('range', lambda y: np.max(y) - np.min(y)),\n",
    "                 ('mean_absolute_change', lambda y: np.mean(np.abs(np.diff(y)))), ('max_jump', lambda y: np.nan if len(np.abs(np.diff(y))) == 0 else np.max(np.abs(np.diff(y)))), # Change dynamics\n",
    "                 ('num_turning_points', lambda y: np.sum(np.diff(np.sign(np.diff(y))) != 0)), ('upward_steps', lambda y:  np.sum(np.diff(y) > 0)), ('downward_steps', lambda y: np.sum(np.diff(y) < 0)), # Complexity\n",
    "                 # ('entropy', lambda y: lambda y: entropy(np.histogram(y, bins=20, density=True)[0])), ('normality', lambda y: normaltest(y)[0]), ('stationarity', lambda y: adfuller(y)[0]), # Distribution shape\n",
    "                 ('avg_fft_mag', lambda y: np.mean(np.abs(np.fft.fft(y)))), ('dominant_freq_ind', lambda y: np.argmax(np.abs(np.fft.fft(y)))), ('sum_fft_mag', lambda y: np.sum(np.abs(np.fft.fft(y)))),  # Frequency\n",
    "                 ('auto_1', lambda y: y.autocorr(lag=1)), ('auto_2', lambda y: y.autocorr(lag=2)), ('auto_3', lambda y: y.autocorr(lag=3))]\n",
    "    statistical_tests = ['pval_ks_2samp_{}', 'pval_ttest_ind_{}', 'pval_levene_{}']\n",
    "    freq_features = ['freq_1_w{}', 'power_1_w{}', 'freq_2_w{}', 'power_2_w{}', 'freq_3_w{}', 'power_3_w{}']\n",
    "\n",
    "    # Find breakpoint time & position difference for each 'id'\n",
    "    break_point_times = df[df['period'] == 1].groupby(['id','period'])[['time']].first()\\\n",
    "                                           .rename({'time': 'breakpoint_time'}, axis=1).reset_index()\n",
    "    df = pd.merge(df, break_point_times[['id', 'breakpoint_time']], on='id', how='inner')\n",
    "    df['position_from_breakpoint_time'] = df['time'] - df['breakpoint_time']\n",
    "    pos = df['position_from_breakpoint_time'].values\n",
    "\n",
    "    # Generate aggrgate features for full range pre and post boundary\n",
    "    pre_break_features = df[df['period'] == 0].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
    "    pre_break_features.columns = [col[0] + '_' + col[1] + '_pre' for col in  pre_break_features.columns]\n",
    "    post_break_features = df[df['period'] == 1].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
    "    post_break_features.columns = [col[0] + '_' + col[1] + '_post' for col in  post_break_features.columns]\n",
    "    pre_break_features['value_slope_pre'] = pre_break_features.apply(lambda x: (x['value_last_pre'] - x['value_first_pre'])/x['value_count_pre'], axis=1)\n",
    "    post_break_features['value_slope_post'] = post_break_features.apply(lambda x: (x['value_last_post'] - x['value_first_post'])/x['value_count_post'], axis=1)\n",
    "\n",
    "    # Statistical features (KS, ttest, levene)\n",
    "    p_values_stat_tests = df.groupby('id').apply(lambda x: extract_statistical_test_features(x, n='full'))\n",
    "    p_values_stat_tests = p_values_stat_tests.droplevel(1)\n",
    "\n",
    "    # Frequency features (Periodogram)\n",
    "    pre_freq_features = df[df['period'] == 0].groupby('id').apply(lambda x: extract_frequency_features(x['value'], n='full', num_main_freqs=3))\n",
    "    pre_freq_features = pre_freq_features.droplevel(1).filter(regex='^value_freq|^value_power')\n",
    "    pre_freq_features.columns = [col + '_pre' for col in pre_freq_features.columns]\n",
    "    post_freq_features = df[df['period'] == 1].groupby('id').apply(lambda x: extract_frequency_features(x['value'], n='full', num_main_freqs=3))\n",
    "    post_freq_features = post_freq_features.droplevel(1).filter(regex='^value_freq|^value_power')\n",
    "    post_freq_features.columns = [col + '_post' for col in post_freq_features.columns]\n",
    "\n",
    "    df_features = pd.concat([pre_break_features, post_break_features, p_values_stat_tests, pre_freq_features, post_freq_features], axis=1)\n",
    "\n",
    "    print('DONE 1: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "\n",
    "    # Calculate interactions for pre & post breakpoint full range (diff, avg, ratio and pct_change)\n",
    "    for agg in [f[0] for f in agg_funcs] + freq_features:\n",
    "        if agg.startswith('freq') or agg.startswith('power'):\n",
    "            agg = agg.format('full')\n",
    "        df_features[f'diff_{agg}'] = df_features[f'value_{agg}_post'] - df_features[f'value_{agg}_pre']\n",
    "        df_features[f'avg_{agg}'] =  (df_features[f'value_{agg}_post'] + df_features[f'value_{agg}_pre'])/2\n",
    "        df_features[f'ratio_{agg}'] =  df_features.apply(lambda x: round(x[f'value_{agg}_post']/x[f'value_{agg}_pre'], 4)\n",
    "                                                                        if x[f'value_{agg}_pre'] != 0 else np.nan, axis=1)\n",
    "        df_features[f'pct_change_{agg}'] = df_features.apply(lambda x: round(x[f'diff_{agg}']/x[f'avg_{agg}'], 4)\n",
    "                                                                        if x[f'avg_{agg}'] != 0 else np.nan, axis=1)\n",
    "\n",
    "    print('DONE 2: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "\n",
    "    # Generate aggrgate features for n-point windows pre and post boundary\n",
    "    for n in tqdm.tqdm(windows):\n",
    "        # Centre windows\n",
    "        pre_break_features = df[df['position_from_breakpoint_time'].between(-n, -1)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
    "        pre_break_features.columns = [col[0] + '_' + col[1] + f'_pre_{n}' for col in  pre_break_features.columns]\n",
    "\n",
    "        post_break_features = df[df['position_from_breakpoint_time'].between(0, n-1)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
    "        post_break_features.columns = [col[0] + '_' + col[1] + f'_post_{n}' for col in  post_break_features.columns]\n",
    "\n",
    "        pre_break_features[f'value_slope_pre_{n}'] = pre_break_features.apply(lambda x: (x[f'value_last_pre_{n}'] - x[f'value_first_pre_{n}'])/x[f'value_count_pre_{n}'], axis=1)\n",
    "        post_break_features[f'value_slope_post_{n}'] = post_break_features.apply(lambda x: (x[f'value_last_post_{n}'] - x[f'value_first_post_{n}'])/x[f'value_count_post_{n}'], axis=1)\n",
    "\n",
    "        # Statistical features (KS, ttest, levene)\n",
    "        p_values_stat_tests = df.groupby('id').apply(lambda x: extract_statistical_test_features(x, n=n))\n",
    "        p_values_stat_tests = p_values_stat_tests.droplevel(1)\n",
    "\n",
    "        # Frequency features (Periodogram)\n",
    "        pre_freq_features = df[(df['position_from_breakpoint_time'].between(-n, -1)) & (df['period'] == 0)].groupby('id').apply(lambda x: extract_frequency_features(x['value'], n=n, num_main_freqs=3))\n",
    "        pre_freq_features = pre_freq_features.droplevel(1).filter(regex='^value_freq|^value_power')\n",
    "        pre_freq_features.columns = [col + '_pre' for col in pre_freq_features.columns]\n",
    "        post_freq_features = df[(df['position_from_breakpoint_time'].between(0, n-1)) & (df['period'] == 1)].groupby('id').apply(lambda x: extract_frequency_features(x['value'], n=n, num_main_freqs=3))\n",
    "        post_freq_features = post_freq_features.droplevel(1).filter(regex='^value_freq|^value_power')\n",
    "        post_freq_features.columns = [col + '_post' for col in post_freq_features.columns]\n",
    "\n",
    "        df_features_window = pd.concat([pre_break_features, post_break_features, p_values_stat_tests, pre_freq_features, post_freq_features], axis=1)\n",
    "\n",
    "        # Outer windows\n",
    "        pre_break_features = df[~df['position_from_breakpoint_time'].between(-n, -1)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
    "        post_break_features = df[~df['position_from_breakpoint_time'].between(0, n-1)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
    "        pre_break_features.columns = [col[0] + '_' + col[1] + f'_pre_{n}_out' for col in  pre_break_features.columns]\n",
    "        post_break_features.columns = [col[0] + '_' + col[1] + f'_post_{n}_out' for col in  post_break_features.columns]\n",
    "\n",
    "        pre_break_features[f'value_slope_pre_{n}_out'] = pre_break_features.apply(lambda x: (x[f'value_last_pre_{n}_out'] - x[f'value_first_pre_{n}_out'])/x[f'value_count_pre_{n}_out'], axis=1)\n",
    "        post_break_features[f'value_slope_post_{n}_out'] = post_break_features.apply(lambda x: (x[f'value_last_post_{n}_out'] - x[f'value_first_post_{n}_out'])/x[f'value_count_post_{n}_out'], axis=1)\n",
    "\n",
    "        # Statistical tests\n",
    "        p_values_stat_tests = df[~df['position_from_breakpoint_time'].between(-n, n-1)].groupby('id').apply(lambda x: extract_statistical_test_features(x, n))\n",
    "        p_values_stat_tests = p_values_stat_tests.droplevel(1)\n",
    "        p_values_stat_tests.columns = [col + '_out' for col in p_values_stat_tests.columns]\n",
    "\n",
    "        # Frequency features (Periodogram)\n",
    "        pre_freq_features = df[(~df['position_from_breakpoint_time'].between(-n, -1)) & (df['period'] == 0)].groupby('id').apply(lambda x: extract_frequency_features(x['value'], n=n, num_main_freqs=3))\n",
    "        pre_freq_features = pre_freq_features.droplevel(1).filter(regex='^value_freq|^value_power')\n",
    "        pre_freq_features.columns = [col + '_pre_out' for col in pre_freq_features.columns]\n",
    "        post_freq_features = df[~(df['position_from_breakpoint_time'].between(0, n-1)) & (df['period'] == 1)].groupby('id').apply(lambda x: extract_frequency_features(x['value'], n=n, num_main_freqs=3))\n",
    "        post_freq_features = post_freq_features.droplevel(1).filter(regex='^value_freq|^value_power')\n",
    "        post_freq_features.columns = [col + '_post_out' for col in post_freq_features.columns]\n",
    "\n",
    "        df_features_window_out = pd.concat([df_features, pre_break_features, post_break_features, p_values_stat_tests, pre_freq_features, post_freq_features], axis=1)\n",
    "\n",
    "        # print('DONE 3: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "\n",
    "        # Calculate interactions for pre & post breakpoint (diff, avg, ratio and pct_change)\n",
    "        for agg in [f[0] for f in agg_funcs] + freq_features:\n",
    "            # Centre windows\n",
    "            if agg.startswith('freq') or agg.startswith('power'):\n",
    "                agg = agg.format(n)\n",
    "                df_features_window[f'diff_{agg}'] = df_features_window[f'value_{agg}_post'] - df_features_window[f'value_{agg}_pre']\n",
    "                df_features_window[f'avg_{agg}'] =  (df_features_window[f'value_{agg}_post'] + df_features_window[f'value_{agg}_pre'])/2\n",
    "                df_features_window[f'ratio_{agg}'] =  df_features_window.apply(lambda x: round(x[f'value_{agg}_post']/x[f'value_{agg}_pre'], 4)\n",
    "                                                                            if x[f'value_{agg}_pre'] != 0 else np.nan, axis=1)\n",
    "                df_features_window[f'pct_change_{agg}'] = df_features_window.apply(lambda x: round(x[f'diff_{agg}']/x[f'avg_{agg}'], 4)\n",
    "                                                                                if x[f'avg_{agg}'] != 0 else np.nan, axis=1)\n",
    "            else:\n",
    "                df_features_window[f'diff_{agg}_{n}'] = df_features_window[f'value_{agg}_post_{n}'] - df_features_window[f'value_{agg}_pre_{n}']\n",
    "                df_features_window[f'avg_{agg}_{n}'] =  (df_features_window[f'value_{agg}_post_{n}'] + df_features_window[f'value_{agg}_pre_{n}'])/2\n",
    "                df_features_window[f'ratio_{agg}_{n}'] =  df_features_window.apply(lambda x: round(x[f'value_{agg}_post_{n}']/x[f'value_{agg}_pre_{n}'], 4)\n",
    "                                                                                if x[f'value_{agg}_pre_{n}'] != 0 else np.nan, axis=1)\n",
    "                df_features_window[f'pct_change_{agg}_{n}'] = df_features_window.apply(lambda x: round(x[f'diff_{agg}_{n}']/x[f'avg_{agg}_{n}'], 4)\n",
    "                                                                                if x[f'avg_{agg}_{n}'] != 0 else np.nan, axis=1)\n",
    "\n",
    "                # Outer windows\n",
    "                df_features[f'diff_{agg}_{n}_out'] = df_features[f'value_{agg}_post_{n}_out'] - df_features[f'value_{agg}_pre_{n}_out']\n",
    "                df_features[f'avg_{agg}_{n}_out'] =  (df_features[f'value_{agg}_post_{n}_out'] + df_features[f'value_{agg}_pre_{n}_out'])/2\n",
    "                df_features[f'ratio_{agg}_{n}_out'] =  df_features.apply(lambda x: round(x[f'value_{agg}_post_{n}_out']/x[f'value_{agg}_pre_{n}_out'], 4)\n",
    "                                                                                if x[f'value_{agg}_pre_{n}_out'] != 0 else np.nan, axis=1)\n",
    "                df_features[f'pct_change_{agg}_{n}_out'] = df_features.apply(lambda x: round(x[f'diff_{agg}_{n}_out']/x[f'avg_{agg}_{n}_out'], 4)\n",
    "                                                                                if x[f'avg_{agg}_{n}_out'] != 0 else np.nan, axis=1)\n",
    "        df_features = pd.concat([df_features, df_features_window, df_features_window_out], axis=1)\n",
    "\n",
    "    print('DONE 3: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "\n",
    "    # Select final feature set\n",
    "    feature_cols = df_features.filter(regex='^value\\_|^diff\\_|^avg\\_|^ratio\\_|^freq.*|^pval.*').columns\n",
    "    df_features = df_features[feature_cols]\n",
    "    df_features = df_features.copy()\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18j52qYygst8"
   },
   "outputs": [],
   "source": [
    "# Sliding windows\n",
    "# Generate aggrgate features for n-point window pre and post boundary\n",
    "# for n in tqdm.tqdm(range(1, 21, 1)):\n",
    "#     pre_break_features = df[df['position_from_breakpoint_time'].between(-50*n, -50*(n-1))].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
    "#     pre_break_features.columns = [col[0] + '_' + col[1] + f'_pre_{n}_mw' for col in  pre_break_features.columns]\n",
    "#     post_break_features = df[df['position_from_breakpoint_time'].between(50*(n-1), 50*n)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
    "#     post_break_features.columns = [col[0] + '_' + col[1] + f'_post_{n}_mw' for col in  post_break_features.columns]\n",
    "#     pre_break_features[f'value_slope_pre_{n}_mw'] = pre_break_features.apply(lambda x: (x[f'value_last_pre_{n}_mw'] - x[f'value_first_pre_{n}_mw'])/x[f'value_count_pre_{n}_mw'], axis=1)\n",
    "#     post_break_features[f'value_slope_post_{n}_mw'] = post_break_features.apply(lambda x: (x[f'value_last_post_{n}_mw'] - x[f'value_first_post_{n}_mw'])/x[f'value_count_post_{n}_mw'], axis=1)\n",
    "\n",
    "#     df_features = pd.concat([df_features, pre_break_features, post_break_features], axis=1)\n",
    "\n",
    "# Statistical tests\n",
    "# for n in tqdm.tqdm(range(1, 21, 1)):\n",
    "#     p_values_stat_tests = df[df['position_from_breakpoint_time'].between(-n, n-1)].groupby('id').apply(lambda x: extract_statistical_test_features(x, n))\n",
    "#     p_values_stat_tests = p_values_stat_tests.droplevel(1)\n",
    "#     df_features = pd.concat([df_features, p_values_stat_tests], axis=1)\n",
    "\n",
    "# Calculate interactions for pre & post breakpoint (diff, avg, ratio and pct_change)\n",
    "# for agg in [f[0] for f in agg_funcs]:\n",
    "#     df_features[f'diff_{agg}_{n}_mw'] = df_features[f'value_{agg}_post_{n}_mw'] - df_features[f'value_{agg}_pre_{n}_mw']\n",
    "#     df_features[f'avg_{agg}_{n}_mw'] =  (df_features[f'value_{agg}_post_{n}_mw'] + df_features[f'value_{agg}_pre_{n}_mw'])/2\n",
    "#     df_features[f'ratio_{agg}_{n}_mw'] =  df_features.apply(lambda x: round(x[f'value_{agg}_post_{n}_mw']/x[f'value_{agg}_pre_{n}_mw'], 4)\n",
    "#                                                                        if x[f'value_{agg}_pre_{n}_mw'] != 0 else np.nan, axis=1)\n",
    "#     df_features[f'pct_change_{agg}_{n}_mw'] = df_features.apply(lambda x: round(x[f'diff_{agg}_{n}_mw']/x[f'avg_{agg}_{n}_mw'], 4)\n",
    "#                                                                        if x[f'avg_{agg}_{n}_mw'] != 0 else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93bdpiA_CGpf"
   },
   "outputs": [],
   "source": [
    "def feature_importance(X_train: pd.DataFrame, y_train: pd.Series, k=150):\n",
    "    model = XGBClassifier(n_estimators=500, objective='binary:logistic', max_depth=6, n_jobs=-1, random_state=15)\n",
    "    print('TRAINING MODEL :')\n",
    "    model.fit(X_train_features, y_train.values)\n",
    "\n",
    "    feat_imp = pd.DataFrame({\n",
    "        \"feature\": X_train_features.columns,\n",
    "        \"importance\": model.feature_importances_})\n",
    "\n",
    "    # Sort by importance (descending)\n",
    "    feat_imp = feat_imp.sort_values(by=\"importance\", ascending=False)\n",
    "    best_features = feat_imp[:k]['feature']\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nm4QrLaTXSjj"
   },
   "outputs": [],
   "source": [
    "# # Generate featrures from train data\n",
    "X_train_features = generate_features(X_train.reset_index())\n",
    "assert len(list(X_train_features.columns)) == len(set(X_train_features.columns))\n",
    "\n",
    "# Select top-k best features for training\n",
    "best_features = feature_importance(X_train_features, y_train, k=150)\n",
    "\n",
    "# Select best params for model training\n",
    "best_params = {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQwWDC6M-0fT"
   },
   "outputs": [],
   "source": [
    "def train(X_train: pd.DataFrame, y_train: pd.Series, model_directory_path: str):\n",
    "    # Train classifier model to predict 'period'\n",
    "    model = XGBClassifier(**best_params, objective='binary:logistic', n_jobs=-1, random_state=15)\n",
    "    print('TRAINING MODEL :')\n",
    "    model.fit(X_train_features[best_features], y_train.values)\n",
    "\n",
    "    # Get prediction from trained model\n",
    "    # y_train_prediction = model.predict(X_train_features)\n",
    "    # train_auc = sklearn.metrics.roc_auc_score(y_train['structural_breakpoint'], y_train_prediction)\n",
    "\n",
    "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n-jboJH-0fU"
   },
   "source": [
    "### The `infer()` Function\n",
    "\n",
    "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
    "\n",
    "**Important workflow:**\n",
    "1. Load your model;\n",
    "2. Use the `yield` statement to signal readiness to the runner;\n",
    "3. Process each dataset one by one within the for loop;\n",
    "4. For each dataset, use `yield prediction` to return your prediction.\n",
    "\n",
    "**Note:** The datasets can only be iterated once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZlEQkSK_H3H"
   },
   "outputs": [],
   "source": [
    "# Infer structural break based on trained model predictions\n",
    "def infer(\n",
    "    X_test: typing.Iterable[pd.DataFrame],\n",
    "    model_directory_path: str,\n",
    "):\n",
    "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
    "    yield  # Mark as ready\n",
    "\n",
    "    # X_test can only be iterated once.\n",
    "    # Before getting the next dataset, you must predict the current one.\n",
    "    for dataset in X_test:\n",
    "        def predict(df_features: pd.DataFrame):\n",
    "            prob = model.predict_proba(df_features)[:, 1]\n",
    "            return prob\n",
    "\n",
    "        dataset = dataset.reset_index()\n",
    "\n",
    "        # Generate features from values before and after breakpoint\n",
    "        X_test_features = generate_features(dataset)\n",
    "\n",
    "        prediction = predict(X_test_features[best_features])\n",
    "        yield prediction  # Send the prediction for the current dataset\n",
    "\n",
    "        # Note: This baseline approach uses a t-test to compare the distributions\n",
    "        # before and after the boundary point. A smaller p-value (larger negative number)\n",
    "        # suggests stronger evidence that the distributions are different,\n",
    "        # indicating a potential structural break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkMI-IOd4Qez"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W0Kl9CA-0fU"
   },
   "source": [
    "## Local testing\n",
    "\n",
    "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
    "Even if it is not perfect, it should give you a quick idea if your model is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDZeP-4--0fU"
   },
   "outputs": [],
   "source": [
    "crunch.test(\n",
    "    # Uncomment to disable the train\n",
    "    # force_first_train=False,\n",
    "\n",
    "    # Uncomment to disable the determinism check\n",
    "    # no_determinism_check=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV_5CKs--0fU"
   },
   "source": [
    "## Results\n",
    "\n",
    "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ly5q68sA-0fU"
   },
   "outputs": [],
   "source": [
    "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oP-NLGh-0fU"
   },
   "source": [
    "### Local scoring\n",
    "\n",
    "You can call the function that the system uses to estimate your score locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyCrjpzv-0fU"
   },
   "outputs": [],
   "source": [
    "# Load the targets\n",
    "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
    "\n",
    "# Call the scoring function\n",
    "sklearn.metrics.roc_auc_score(\n",
    "    target,\n",
    "    prediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AE1i3pR-0fV"
   },
   "source": [
    "# Submit your Notebook\n",
    "\n",
    "To submit your work, you must:\n",
    "1. Download your Notebook from Colab\n",
    "2. Upload it to the platform\n",
    "3. Create a run to validate it\n",
    "\n",
    "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
    "\n",
    "![Download and Submit Notebook](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/download-and-submit-notebook.gif)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
