{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet(\"data/X_train.parquet\")\n",
    "X_train_features = pd.read_parquet(\"data/X_train_features.parquet\")\n",
    "y_train = pd.read_parquet(\"data/y_train.parquet\")\n",
    "\n",
    "# X_test = pd.read_parquet(\"X_test.reduced.parquet\")\n",
    "# y_test = pd.read_parquet(\"y_test.reduced.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def piecewise_ar_break_test(series, break_point, segment_length_1=None, segment_length_2=None, max_lag=10):\n",
    "    \"\"\"\n",
    "    Detect structural break at break_point using piecewise AR modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - series: 1D numpy array or list of time series values\n",
    "    - break_point: index where the series is split\n",
    "    - max_lag: maximum lag to consider for AR models\n",
    "    \n",
    "    Returns:\n",
    "    - dict with AICs, log-likelihoods, and p-value of likelihood ratio test\n",
    "    \"\"\"\n",
    "    series = np.array(series)\n",
    "    s1, s2 = series[:break_point], series[break_point:]\n",
    "    \n",
    "    if segment_length_1:\n",
    "        s1 = s1[-segment_length_1:]\n",
    "    if segment_length_2:\n",
    "        s2 = s2[:segment_length_2]\n",
    "\n",
    "#     # Fit AR models\n",
    "#     model_full = AutoReg(series, lags=max_lag, old_names=False).fit()\n",
    "#     model_s1 = AutoReg(s1, lags=max_lag, old_names=False).fit()\n",
    "#     model_s2 = AutoReg(s2, lags=max_lag, old_names=False).fit()\n",
    "  \n",
    "    # Fit AUTO-ARIMA models\n",
    "    model_full = auto_arima(series, start_p=0, start_q=0, max_p=3, max_q=3,\n",
    "                   m=1, d=None, seasonal=False, stepwise=True, trace=False, \n",
    "                   error_action='ignore', suppress_warnings=True)\n",
    "    model_s1 = auto_arima(s1, start_p=0, start_q=0, max_p=3, max_q=3,\n",
    "                           m=1, d=None, seasonal=False, stepwise=True, trace=False, \n",
    "                           error_action='ignore', suppress_warnings=True)\n",
    "    model_s2 = auto_arima(s2, start_p=0, start_q=0, max_p=3, max_q=3,\n",
    "                       m=1, d=None, seasonal=False, stepwise=True, trace=False, \n",
    "                       error_action='ignore', suppress_warnings=True)\n",
    "\n",
    "    # Compare log-likelihoods\n",
    "    ll_full = model_full.arima_res_.llf\n",
    "    ll_piecewise = model_s1.arima_res_.llf + model_s2.arima_res_.llf\n",
    "\n",
    "    # Likelihood ratio test\n",
    "    lr_stat = 2 * (ll_piecewise - ll_full)\n",
    "    df = model_s1.arima_res_.df_model + model_s2.arima_res_.df_model - model_full.arima_res_.df_model\n",
    "    p_value = chi2.sf(lr_stat, df)\n",
    "\n",
    "    return {\n",
    "        'AIC_full': model_full.arima_res_.aic,\n",
    "        'AIC_piecewise': model_s1.arima_res_.aic + model_s2.arima_res_.aic,\n",
    "        'LL_full': ll_full,\n",
    "        'LL_piecewise': ll_piecewise,\n",
    "        'LR_stat': lr_stat,\n",
    "        'df': df,\n",
    "        'p_value': p_value\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simulate time series with a break\n",
    "np.random.seed(42)\n",
    "predictions = []\n",
    "for i in tqdm.tqdm(range(0, 10001)):\n",
    "    series = X_train.loc[i].reset_index()\n",
    "    break_point = series.groupby('period')['time'].first()[1]\n",
    "\n",
    "    # Run break test\n",
    "    result = piecewise_ar_break_test(series['value'], break_point=break_point, segment_length_1=None, segment_length_2=None, max_lag=3)\n",
    "    predictions.append(result)\n",
    "    \n",
    "df_pred = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate roc-auc score for predictions\n",
    "# df_pred['prediction'] = df_pred['p_value'].apply(lambda p: False if p >= 0.05 else True)\n",
    "df_pred['prediction_1'] = df_pred.apply(lambda x: x['LL_full'] - x['LL_piecewise'], axis=1)\n",
    "\n",
    "target = y_train['structural_breakpoint'].replace({False: 0, True: 1})\n",
    "roc_auc_score(target[:507], df_pred['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESULTS\n",
    "# 1. AUTO_REG (Train : 10,000 samples) :- 0.54\n",
    "# 2. AUTO_ARIMA (Train : 500 samples) :-  0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_coefficient_match(param):\n",
    "    conf_int = 0.05\n",
    "    if param['pval_coef_period_0'] > conf_int:\n",
    "        param ['coef_period_0'] = np.nan\n",
    "     \n",
    "    if param['pval_coef_period_1'] > conf_int:\n",
    "        param ['coef_period_1'] = np.nan\n",
    "\n",
    "    if pd.isna(param['coef_period_0']) or pd.isna(param['coef_period_1']):\n",
    "        return False\n",
    "    elif (param['coef_period_0'] > param['coef_period_1']) and (param['0.975_period_1'] < param['0.025_period_0']):\n",
    "        return False\n",
    "    elif (param['coef_period_0'] > param['coef_period_1']) and (param['0.975_period_1'] >= param['0.025_period_0']):\n",
    "        return True\n",
    "    elif (param['coef_period_0'] < param['coef_period_1']) and (param['0.975_period_0'] < param['0.025_period_1']):\n",
    "        return False\n",
    "    elif (param['coef_period_0'] < param['coef_period_1']) and (param['0.975_period_0'] >= param['0.025_period_1']):\n",
    "        return True\n",
    "    elif param['coef_period_0'] == param['coef_period_1']:\n",
    "        return True\n",
    "\n",
    "def fit_and_compare_arima(data, label, id, n_exceptions):\n",
    "    period_0 = data.loc[data['period'] == 0].reset_index()\n",
    "    period_1 = data.loc[data['period'] == 1].reset_index()\n",
    "    \n",
    "    model_0 = auto_arima(period_0['value'], start_p=0, start_q=0, max_p=3, max_q=3,\n",
    "                       m=1, d=None, seasonal=False, stepwise=True, trace=False, \n",
    "                       error_action='ignore', suppress_warnings=True)\n",
    "    model_1 = auto_arima(period_1['value'], start_p=0, start_q=0, max_p=3, max_q=3,\n",
    "                           m=1, d=None, seasonal=False, stepwise=True, trace=False, \n",
    "                           error_action='ignore', suppress_warnings=True)\n",
    "    model_0_params = pd.concat([model_0.arima_res_.params.to_frame().rename({0: 'coef_period_0'}, axis=1),\n",
    "                            model_0.arima_res_.conf_int().rename({0: '0.025_period_0', 1: '0.975_period_0'}, axis=1),\n",
    "                            model_0.arima_res_.pvalues.to_frame().rename({0: 'pval_coef_period_0'}, axis=1)], axis=1)\n",
    "    model_1_params = pd.concat([model_1.arima_res_.params.to_frame().rename({0: 'coef_period_1'}, axis=1),\n",
    "                            model_1.arima_res_.conf_int().rename({0: '0.025_period_1', 1: '0.975_period_1'}, axis=1),\n",
    "                            model_1.arima_res_.pvalues.to_frame().rename({0: 'pval_coef_period_1'}, axis=1)], axis=1)\n",
    "    model_params = pd.concat([model_0_params, model_1_params], axis=1)  \n",
    "    model_params['coeff_match'] = model_params.apply(lambda row: get_coefficient_match(row), axis=1)\n",
    "    model_params = model_params.assign(structural_breakpoint = label['structural_breakpoint'], id = id)\n",
    "    model_params = model_params.reset_index().rename(columns={'index': 'coefficient'})\n",
    "\n",
    "    threshold = 0.9\n",
    "    try:\n",
    "        result = pd.DataFrame({'id': id, 'structural_breakpoint': label['structural_breakpoint'], 'structural_breakpoint_pred': model_params['coeff_match'].value_counts(normalize=True).loc[False]}, index=[0])\n",
    "        return result, model_params\n",
    "    except:\n",
    "        n_exceptions += 1\n",
    "        result = pd.DataFrame({'id': id, 'structural_breakpoint': label['structural_breakpoint'],  'structural_breakpoint_pred': 0.5}, index=[0])\n",
    "        return result, model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame()\n",
    "df_params = pd.DataFrame()\n",
    "n_exceptions = 0\n",
    "\n",
    "for id in tqdm.tqdm(range(10001, 10102)):\n",
    "    df = test_data.loc[id]\n",
    "    label = test_labels.loc[id]\n",
    "\n",
    "    prediction, params = fit_and_compare_arima(df, label, id, n_exceptions)\n",
    "    df_predictions = pd.concat([df_predictions, prediction], axis=0, ignore_index=True)\n",
    "    df_params = pd.concat([df_params, params], axis=0, ignore_index=True)\n",
    "\n",
    "print(n_exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_params[df_params['id'] == 10002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data.loc[10002, 'value'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "# cm = confusion_matrix(df_predictions['structural_breakpoint'], df_predictions['structural_breakpoint_pred'])\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= [False, True])\n",
    "# disp.plot(cmap=\"Blues\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the ROC-AUC score from coefficient match based prediction\n",
    "roc_auc_score(df_predictions['structural_breakpoint'], df_predictions['structural_breakpoint_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefficient_features = ['coef_period_0', '0.025_period_0', '0.975_period_0', 'pval_coef_period_0',\n",
    "                        'coef_period_1', '0.025_period_1', '0.975_period_1', 'pval_coef_period_1'\n",
    "                        'coeff_match']\n",
    "\n",
    "df_params_features = pd.pivot_table(df_params, index=['id'], columns=['coefficient'], values=coefficient_features)\n",
    "df_params_features.columns = ['_'.join(col).strip() for col in df_params_features.columns.values]\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_params_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit XGBoost model with AUTO-Arima coefficients\n",
    "xgb_clf = XGBClassifier(n_estimators=500)\n",
    "\n",
    "# Get the ROC-AUC score from model\n",
    "auc_scores = cross_val_score(xgb_clf, df_params_features, labels[:1000], cv=5, scoring='roc_auc')\n",
    "print(f\"Individual AUC scores: {auc_scores}\")\n",
    "print(f\"Mean AUC score: {np.mean(auc_scores):.4f}\")\n",
    "print(f\"Standard deviation of AUC scores: {np.std(auc_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer structural break based on comparison of parameters\n",
    "def infer(\n",
    "    X_test: typing.Iterable[pd.DataFrame],\n",
    "    model_directory_path: str,\n",
    "):\n",
    "    # model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
    "\n",
    "    yield  # Mark as ready\n",
    "\n",
    "    # X_test can only be iterated once.\n",
    "    # Before getting the next dataset, you must predict the current one.\n",
    "    for dataset in X_test:\n",
    "        # Baseline approach: Compute t-test between values before and after boundary point\n",
    "        # The negative p-value is used as our score - smaller p-values (larger negative numbers)\n",
    "        # indicate more evidence against the null hypothesis that distributions are the same,\n",
    "        # suggesting a structural break\n",
    "        \n",
    "        def predict(df_features: pd.DataFrame):\n",
    "            prediction = fit_and_compare_arima(df_features)\n",
    "            return prediction\n",
    "            \n",
    "        dataset = dataset.reset_index()\n",
    "        prediction = predict(dataset)\n",
    "        yield prediction  # Send the prediction for the current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values = pd.concat([period_0['value'], period_1['value']], axis=0).reset_index()\n",
    "values['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.concat([model_0.predict_in_sample(), model_1.predict_in_sample()], axis=0).reset_index()\n",
    "predictions['predicted_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "values['value'].plot(color='r', ax=ax)\n",
    "predictions['predicted_mean'].plot(color='b', ax=ax)\n",
    "plt.axvline(x = 2407, color='r', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def residual_features(time_series_data, n, max_lag=10):\n",
    "    \"\"\"\n",
    "    Extract residual-based features from two time series segments.\n",
    "    \n",
    "    Parameters:\n",
    "    - segment_1: array-like, first part of the time series\n",
    "    - segment_2: array-like, second part of the time series\n",
    "    - max_lag: lag order for AR model\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of residual-based features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    segment_1 = time_series_data[time_series_data['period'] == 0]['value']\n",
    "    segment_2 = time_series_data[time_series_data['period'] == 1]['value']\n",
    "\n",
    "\n",
    "    # Fit AR models\n",
    "    model_1 = AutoReg(segment_1, lags=max_lag, old_names=False).fit()\n",
    "    model_2 = AutoReg(segment_2, lags=max_lag, old_names=False).fit()\n",
    "\n",
    "    resid_1 = model_1.resid\n",
    "    resid_2 = model_2.resid\n",
    "\n",
    "    # Residual statistics\n",
    "    features[f'resid_mean_1_{n}'] = np.mean(resid_1)\n",
    "    features[f'resid_mean_2_{n}'] = np.mean(resid_2)\n",
    "    features[f'resid_var_1_{n}'] = np.var(resid_1)\n",
    "    features[f'resid_var_2_{n}'] = np.var(resid_2)\n",
    "    features[f'resid_var_ratio_{n}'] = features[f'resid_var_2_{n}'] / (features[f'resid_var_1_{n}'] + 1e-6)\n",
    "    features[f'resid_var_diff_{n}'] = features[f'resid_var_2_{n}'] - features[f'resid_var_1_{n}']\n",
    "\n",
    "    # Autocorrelation of residuals\n",
    "    features[f'resid_acf_1_lag1_{n}'] = pd.Series(resid_1).autocorr(lag=1)\n",
    "    features[f'resid_acf_2_lag1_{n}'] = pd.Series(resid_2).autocorr(lag=1)\n",
    "\n",
    "#     # Ljung–Box test\n",
    "#     lb_1 = acorr_ljungbox(resid_1, lags=[max_lag], return_df=True)\n",
    "#     lb_2 = acorr_ljungbox(resid_2, lags=[max_lag], return_df=True)\n",
    "#     features[f'ljung_box_stat_1_{n}'] = lb_1['lb_stat'].values[0]\n",
    "#     features[f'ljung_box_stat_2_{n}'] = lb_2['lb_stat'].values[0]\n",
    "#     features[f'ljung_box_pval_1_{n}'] = lb_1['lb_pvalue'].values[0]\n",
    "#     features[f'ljung_box_pval_2_{n}'] = lb_2['lb_pvalue'].values[0]\n",
    "\n",
    "#     # Entropy of residuals\n",
    "#     hist_1 = np.histogram(resid_1, bins=20, density=True)[0]\n",
    "#     hist_2 = np.histogram(resid_2, bins=20, density=True)[0]\n",
    "#     features[f'resid_entropy_1_{n}'] = entropy(hist_1 + 1e-6)\n",
    "#     features[f'resid_entropy_2_{n}'] = entropy(hist_2 + 1e-6)\n",
    "#     features[f'resid_entropy_diff_{n}'] = features[f'resid_entropy_2_{n}'] - features[f'resid_entropy_1_{n}']\n",
    "\n",
    "    return pd.DataFrame(features, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find breakpoint time & position difference for each 'id'\n",
    "df = X_train.reset_index()\n",
    "break_point_times = df[df['period'] == 1].groupby(['id','period'])[['time']].first()\\\n",
    "                                       .rename({'time': 'breakpoint_time'}, axis=1).reset_index()\n",
    "df = pd.merge(df, break_point_times[['id', 'breakpoint_time']], on='id', how='inner')\n",
    "df['position_from_breakpoint_time'] = df['time'] - df['breakpoint_time']\n",
    "pos = df['position_from_breakpoint_time'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [w for w in range(50, 1001, 50)]\n",
    "\n",
    "df_resid_features = pd.DataFrame()\n",
    "for n in tqdm.tqdm(windows):\n",
    "    features = df[~df['position_from_breakpoint_time'].between(-n, n-1)].groupby('id').apply(lambda x: residual_features(x, n, max_lag=5))\n",
    "    df_resid_features = pd.concat([df_resid_features, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['value_max_jump_pre_600', 'pval_levene_40', 'value_mean_absolute_change_post_650', 'value_ptp_pre_150', 'value_kurtosis_post_850', 'value_mean_post', 'value_min_post_600', 'value_percentile_25_pre_750', 'value_avg_fft_mag_pre_20', 'pval_ks_2samp_200', 'value_downward_steps_pre_850', 'value_max_post', 'value_kurtosis_pre', 'value_percentile_25_pre_850', 'value_percentile_25_post_450', 'pval_levene_100', 'value_mean_absolute_change_pre_250', 'value_min_post_500', 'value_percentile_25_post_250', 'value_num_turning_points_post_800', 'value_max_post_300', 'value_avg_fft_mag_pre_350', 'value_ptp_post_50', 'value_percentile_25_pre_150', 'value_kurtosis_pre_600', 'value_std_post_60', 'value_sum_fft_mag_post_750', 'value_kurtosis_pre_700', 'value_mean_pre_550', 'value_std_pre_20', 'value_avg_fft_mag_post', 'value_volatility_pre_30', 'pval_ks_2samp_90', 'value_mean_absolute_change_post_50', 'pval_ks_2samp_950', 'value_auto_1_post', 'value_auto_2_post', 'value_slope_post_350', 'value_mean_absolute_change_pre_700', 'value_std_pre_700', 'value_kurtosis_post_600', 'value_min_pre_70', 'value_mean_absolute_change_pre_60', 'value_percentile_25_pre_80', 'value_avg_fft_mag_post_650', 'pval_ks_2samp_30', 'value_mean_pre_450', 'value_max_pre_300', 'value_auto_2_pre_900', 'value_percentile_25_post_90', 'value_percentile_25_pre', 'value_min_pre_950', 'value_mean_absolute_change_post_90', 'value_max_post_40', 'value_num_turning_points_pre_10', 'value_upward_steps_pre_950', 'value_ptp_post_250', 'value_avg_fft_mag_post_500', 'value_ptp_pre_250', 'value_std_post_10', 'value_ptp_post_600', 'value_downward_steps_post_200', 'value_sum_fft_mag_post_400', 'value_downward_steps_post', 'value_mean_post_500', 'value_auto_1_post_450', 'value_max_jump_pre_70', 'value_num_turning_points_pre_200', 'pval_levene_400', 'value_mean_post_650', 'value_percentile_25_post_150', 'value_skew_post_800', 'value_std_post_20', 'value_ptp_post_5', 'diff_max_jump', 'pval_ks_2samp_60', 'value_mean_absolute_change_pre_450', 'value_auto_3_pre_350', 'ratio_mean_absolute_change', 'diff_mean', 'value_ptp_pre_500', 'pval_levene_30', 'value_num_turning_points_post_80', 'value_volatility_post_90', 'value_ptp_pre_700', 'value_upward_steps_post_90', 'value_upward_steps_pre_250', 'value_percentile_25_pre_250', 'value_volatility_post_300', 'value_num_turning_points_post_50', 'value_max_jump_pre_10', 'value_volatility_pre_20', 'value_dominant_freq_ind_post_800', 'value_ptp_pre_550', 'value_upward_steps_pre_70', 'value_std_pre_500', 'ratio_std_1000', 'pval_ks_2samp_150', 'pval_ks_2samp_500', 'value_ptp_pre_950', 'value_percentile_75_post_450', 'value_std_post_80', 'pval_ttest_ind_950', 'value_volatility_pre_700', 'value_ptp_pre_650', 'value_upward_steps_post', 'pval_ks_2samp_700', 'value_upward_steps_pre_60', 'value_auto_1_pre_800', 'ratio_mean_absolute_change_1000', 'value_skew_post_400', 'value_auto_3_post_550', 'value_mean_pre_350', 'value_num_turning_points_pre_250', 'pval_ks_2samp_5', 'pval_levene_80', 'value_auto_1_pre_700', 'value_downward_steps_post_500', 'value_trend_post_350', 'ratio_num_turning_points', 'value_volatility_pre_750', 'value_median_pre_250', 'value_trend_post_80', 'value_skew_post', 'value_max_pre_80', 'value_ptp_post_750', 'value_mean_absolute_change_post_60', 'value_avg_fft_mag_pre', 'value_mean_pre_800', 'value_median_pre_150', 'value_percentile_75_pre_600', 'ratio_median', 'pval_ks_2samp_20', 'value_skew_pre_40', 'diff_percentile_25', 'value_trend_pre_150', 'value_kurtosis_post_150', 'value_auto_2_post_550', 'pval_levene_750', 'value_upward_steps_pre_350', 'value_max_jump_pre', 'value_mean_absolute_change_pre_650', 'value_percentile_25_post_500', 'pval_ks_2samp_550', 'value_auto_2_post_300', 'value_max_jump_post_250', 'diff_num_turning_points_1000', 'value_ptp_pre', 'value_mean_absolute_change_pre_20', 'value_dominant_freq_ind_post_950']\n",
    "X_tr = X_train_features[best_features].join(df_resid_features.droplevel(1))\n",
    "X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=1000, objective='binary:logistic',\n",
    "                      max_depth=8, learning_rate=0.1, n_jobs=-1, random_state=15)\n",
    "\n",
    "print('RUNNING CROSS VALIDATION :')\n",
    "roc_auc_scores = cross_val_score(model, X_tr, y_train.values, cv=5, scoring='roc_auc', verbose=1)\n",
    "print(roc_auc_scores)\n",
    "print(roc_auc_scores.mean(), roc_auc_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.705855 0.72442352 0.73095425 0.72545912 0.7344343]\n",
    "0.7242252373443077 0.009884653564360298\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
