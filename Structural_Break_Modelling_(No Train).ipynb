{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWIItAe-0fN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/crunchdao/quickstarters/blob/master/competitions/structural-break/quickstarters/baseline/baseline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNUXnJa_-0fO"
      },
      "source": [
        "![Banner](https://raw.githubusercontent.com/crunchdao/quickstarters/refs/heads/master/competitions/structural-break/assets/banner.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurIF1Ve-0fP"
      },
      "source": [
        "# ADIA Lab Structural Break Challenge\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "## Challenge Overview\n",
        "\n",
        "Welcome to the ADIA Lab Structural Break Challenge! In this challenge, you will analyze univariate time series data to determine whether a structural break has occurred at a specified boundary point.\n",
        "\n",
        "### What is a Structural Break?\n",
        "\n",
        "A structural break occurs when the process governing the data generation changes at a certain point in time. These changes can be subtle or dramatic, and detecting them accurately is crucial across various domains such as climatology, industrial monitoring, finance, and healthcare.\n",
        "\n",
        "![Structural Break Example](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/structural-break/quickstarters/baseline/images/example.png)\n",
        "\n",
        "### Your Task\n",
        "\n",
        "For each time series in the test set, you need to predict a score between `0` and `1`:\n",
        "- Values closer to `0` indicate no structural break at the specified boundary point;\n",
        "- Values closer to `1` indicate a structural break did occur.\n",
        "\n",
        "### Evaluation Metric\n",
        "\n",
        "The evaluation metric is [ROC AUC (Area Under the Receiver Operating Characteristic Curve)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), which measures the performance of detection algorithms regardless of their specific calibration.\n",
        "\n",
        "- ROC AUC around `0.5`: No better than random chance;\n",
        "- ROC AUC approaching `1.0`: Perfect detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Lvv2Pps-fT"
      },
      "source": [
        "# Setup\n",
        "\n",
        "The first steps to get started are:\n",
        "1. Get the setup command\n",
        "2. Execute it in the cell below\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Reveal token](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/reveal-token.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DUeixiC_IJM"
      },
      "outputs": [],
      "source": [
        "%pip install crunch-cli --upgrade --quiet --progress-bar off\n",
        "!crunch setup-notebook structural-break HBS3kIvMp1h4YLvQTBuMZTNB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IBhw7hv-0fQ"
      },
      "source": [
        "# Your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpLeMWSw-0fQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "import tqdm\n",
        "import logging\n",
        "\n",
        "# Import your dependencies\n",
        "import joblib\n",
        "import numpy as np # == 1.26.4\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from scipy.stats import ttest_ind, ks_2samp, levene\n",
        "from scipy.stats import entropy, normaltest, jarque_bera\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from scipy.signal import periodogram\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import sklearn.metrics\n",
        "from xgboost import XGBClassifier\n",
        "from prophet import Prophet\n",
        "# from pmdarima import auto_arima # == 2.0.4\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pI3_XRw8kaV"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "# !pip install numpy==1.26.4\n",
        "# !pip install pmdarima==2.0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjD_WSAS-0fR"
      },
      "outputs": [],
      "source": [
        "import crunch\n",
        "\n",
        "# Load the Crunch Toolings\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiKJODFx-0fR"
      },
      "source": [
        "## Understanding the Data\n",
        "\n",
        "The dataset consists of univariate time series, each containing ~2,000-5,000 values with a designated boundary point. For each time series, you need to determine whether a structural break occurred at this boundary point.\n",
        "\n",
        "The data was downloaded when you setup your local environment and is now available in the `data/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKHXgvjN-0fS"
      },
      "outputs": [],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T_JmgMq-0fS"
      },
      "source": [
        "### Understanding `X_train`\n",
        "\n",
        "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
        "\n",
        "**Index Levels:**\n",
        "- `id`: Identifies the unique time series\n",
        "- `time`: The timestep within each time series\n",
        "\n",
        "**Columns:**\n",
        "- `value`: The actual time series value at each timestep\n",
        "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oRCTnOb-0fS"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP39dgx-0fS"
      },
      "source": [
        "### Understanding `y_train`\n",
        "\n",
        "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
        "\n",
        "**Index:**\n",
        "- `id`: the ID of the dataset\n",
        "\n",
        "**Value:**\n",
        "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPsQPdIj-0fT"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oSS08Ks-0fT"
      },
      "source": [
        "### Understanding `X_test`\n",
        "\n",
        "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
        "\n",
        "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ErbKAs--0fT"
      },
      "outputs": [],
      "source": [
        "print(\"Number of datasets:\", len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_dTYXms-0fT"
      },
      "outputs": [],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgulFOGX-0fT"
      },
      "source": [
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYu5aXF7HIle"
      },
      "outputs": [],
      "source": [
        "# Generate features from values before and after breakpoint\n",
        "def generate_features(df: pd.DataFrame):\n",
        "    # Define number of windows\n",
        "    n_points = df.groupby('period')['id'].count().min()\n",
        "    start = [s for s in range(0, 50, 10)]\n",
        "    windows = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90] + [w for w in range(100, 1001, 50)]\n",
        "    print('STARTED FEATURE GENERATION: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
        "\n",
        "    def extract_statistical_test_features(time_series_data, n):\n",
        "        period_0 = time_series_data[time_series_data['period'] == 0]['value']\n",
        "        period_1 = time_series_data[time_series_data['period'] == 1]['value']\n",
        "        _ , p_value_1 = ks_2samp(period_0, period_1)\n",
        "        _ , p_value_2 = ttest_ind(period_0, period_1)\n",
        "        _ , p_value_3 = levene(period_0, period_1)\n",
        "        p_values_stat_tests = pd.DataFrame({f'pval_ks_2samp_{n}': p_value_1, f'pval_ttest_ind_{n}': p_value_2, f'pval_levene_{n}': p_value_3}, index=[0])\n",
        "        return p_values_stat_tests\n",
        "\n",
        "    def extract_frequency_features(time_series_data, sampling_rate=1, num_main_freqs=3):\n",
        "        \"\"\"\n",
        "        Extracts the main frequencies from time series data using a periodogram.\n",
        "\n",
        "        Args:\n",
        "          time_series_data (np.array): The input time series data.\n",
        "          sampling_rate (float): The sampling rate of the time series data (samples per unit time).\n",
        "          num_main_freqs (int): The number of main frequencies to extract.\n",
        "\n",
        "        Returns:\n",
        "          dict: A dictionary containing the top frequencies, their corresponding periods,\n",
        "                and their power spectral densities.\n",
        "        \"\"\"\n",
        "        # Estimate power spectral density using a periodogram\n",
        "        frequencies, power_spectral_density = periodogram(time_series_data, fs=sampling_rate)\n",
        "\n",
        "        # Get indices for the highest power spectral density values\n",
        "        top_freq_indices = np.argsort(power_spectral_density)[::-1][:num_main_freqs]\n",
        "\n",
        "        # Extract the top frequencies, powers, and calculate periods\n",
        "        main_frequencies = frequencies[top_freq_indices]\n",
        "        main_powers = power_spectral_density[top_freq_indices]\n",
        "        main_periods = 1 / main_frequencies\n",
        "\n",
        "        results = {}\n",
        "        for i in range(num_main_freqs):\n",
        "            results[f'freq_{i+1}'] = main_frequencies[i]\n",
        "            results[f'period_{i+1}'] = main_periods[i]\n",
        "            results[f'power_{i+1}'] = main_powers[i]\n",
        "\n",
        "        df_freq = pd.DataFrame(results, index=[0])\n",
        "        return df_freq\n",
        "\n",
        "    agg_funcs = [('mean', 'mean'), ('std', 'std'), ('max', 'max'), ('min', 'min'), ('median', 'median'), ('skew', 'skew'), ('kurtosis', lambda y: y.kurt()),\n",
        "                 ('ptp', lambda y: np.ptp(y)), ('percentile_25', lambda y: np.percentile(y, 25)), ('percentile_75', lambda y: np.percentile(y, 75)),\n",
        "                 ('count', 'count'), ('first', 'first'), ('last', 'last'),\n",
        "                 ('trend', lambda y: np.mean(np.diff(y))), ('volatility', lambda y: np.std(np.diff(y))), ('range', lambda y: np.max(y) - np.min(y)),\n",
        "                 ('mean_absolute_change', lambda y: np.mean(np.abs(np.diff(y)))), ('max_jump', lambda y: np.nan if len(np.abs(np.diff(y))) == 0 else np.max(np.abs(np.diff(y)))), # Change dynamics\n",
        "                 ('num_turning_points', lambda y: np.sum(np.diff(np.sign(np.diff(y))) != 0)), ('upward_steps', lambda y:  np.sum(np.diff(y) > 0)), ('downward_steps', lambda y: np.sum(np.diff(y) < 0)), # Complexity\n",
        "                 # ('entropy', lambda y: lambda y: entropy(np.histogram(y, bins=20, density=True)[0])), ('normality', lambda y: normaltest(y)[0]), ('stationarity', lambda y: adfuller(y)[0]), # Distribution shape\n",
        "                 ('avg_fft_mag', lambda y: np.mean(np.abs(np.fft.fft(y)))), ('dominant_freq_ind', lambda y: np.argmax(np.abs(np.fft.fft(y)))), ('sum_fft_mag', lambda y: np.sum(np.abs(np.fft.fft(y)))),  # Frequency\n",
        "                 ('auto_1', lambda y: y.autocorr(lag=1)), ('auto_2', lambda y: y.autocorr(lag=2)), ('auto_3', lambda y: y.autocorr(lag=3))]\n",
        "\n",
        "    # Find breakpoint time & position difference for each 'id'\n",
        "    break_point_times = df[df['period'] == 1].groupby(['id','period'])[['time']].first()\\\n",
        "                                           .rename({'time': 'breakpoint_time'}, axis=1).reset_index()\n",
        "    df = pd.merge(df, break_point_times[['id', 'breakpoint_time']], on='id', how='inner')\n",
        "    df['position_from_breakpoint_time'] = df['time'] - df['breakpoint_time']\n",
        "    pos = df['position_from_breakpoint_time'].values\n",
        "\n",
        "    # Generate aggrgate features for full range pre and post boundary\n",
        "    pre_break_features = df[df['period'] == 0].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
        "    pre_break_features.columns = [col[0] + '_' + col[1] + '_pre' for col in  pre_break_features.columns]\n",
        "    post_break_features = df[df['period'] == 1].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
        "    post_break_features.columns = [col[0] + '_' + col[1] + '_post' for col in  post_break_features.columns]\n",
        "    pre_break_features['value_slope_pre'] = pre_break_features.apply(lambda x: (x['value_last_pre'] - x['value_first_pre'])/x['value_count_pre'], axis=1)\n",
        "    post_break_features['value_slope_post'] = post_break_features.apply(lambda x: (x['value_last_post'] - x['value_first_post'])/x['value_count_post'], axis=1)\n",
        "\n",
        "    df_features = pd.concat([pre_break_features, post_break_features], axis=1)\n",
        "\n",
        "    # Calculate diff, ratio and pct_change between pre and post-break periods for aggregate functions\n",
        "    for agg in [f[0] for f in agg_funcs]:\n",
        "        df_features[f'diff_{agg}'] = df_features[f'value_{agg}_post'] - df_features[f'value_{agg}_pre']\n",
        "        df_features[f'avg_{agg}'] =  (df_features[f'value_{agg}_post'] + df_features[f'value_{agg}_pre'])/2\n",
        "        df_features[f'ratio_{agg}'] =  df_features.apply(lambda x: round(x[f'value_{agg}_post']/x[f'value_{agg}_pre'], 4)\n",
        "                                                                         if x[f'value_{agg}_pre'] != 0 else np.nan, axis=1)\n",
        "        df_features[f'pct_change_{agg}'] = df_features.apply(lambda x: round(x[f'diff_{agg}']/x[f'avg_{agg}'], 4)\n",
        "                                                                         if x[f'avg_{agg}'] != 0 else np.nan, axis=1)\n",
        "\n",
        "    print('DONE 1: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
        "\n",
        "    # Generate aggrgate features for n-point window pre and post boundary (sliding windows)\n",
        "    # for n in tqdm.tqdm(range(1, 21, 1)):\n",
        "    #     pre_break_features = df[df['position_from_breakpoint_time'].between(-50*n, -50*(n-1))].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
        "    #     pre_break_features.columns = [col[0] + '_' + col[1] + f'_pre_{n}_mw' for col in  pre_break_features.columns]\n",
        "    #     post_break_features = df[df['position_from_breakpoint_time'].between(50*(n-1), 50*n)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
        "    #     post_break_features.columns = [col[0] + '_' + col[1] + f'_post_{n}_mw' for col in  post_break_features.columns]\n",
        "    #     pre_break_features[f'value_slope_pre_{n}_mw'] = pre_break_features.apply(lambda x: (x[f'value_last_pre_{n}_mw'] - x[f'value_first_pre_{n}_mw'])/x[f'value_count_pre_{n}_mw'], axis=1)\n",
        "    #     post_break_features[f'value_slope_post_{n}_mw'] = post_break_features.apply(lambda x: (x[f'value_last_post_{n}_mw'] - x[f'value_first_post_{n}_mw'])/x[f'value_count_post_{n}_mw'], axis=1)\n",
        "\n",
        "    #     df_features = pd.concat([df_features, pre_break_features, post_break_features], axis=1)\n",
        "\n",
        "    # Calculate diff, ratio and pct_change between pre and post-break periods for aggregate functions\n",
        "    # for agg in [f[0] for f in agg_funcs]:\n",
        "    #     df_features[f'diff_{agg}_{n}_mw'] = df_features[f'value_{agg}_post_{n}_mw'] - df_features[f'value_{agg}_pre_{n}_mw']\n",
        "    #     df_features[f'avg_{agg}_{n}_mw'] =  (df_features[f'value_{agg}_post_{n}_mw'] + df_features[f'value_{agg}_pre_{n}_mw'])/2\n",
        "    #     df_features[f'ratio_{agg}_{n}_mw'] =  df_features.apply(lambda x: round(x[f'value_{agg}_post_{n}_mw']/x[f'value_{agg}_pre_{n}_mw'], 4)\n",
        "    #                                                                        if x[f'value_{agg}_pre_{n}_mw'] != 0 else np.nan, axis=1)\n",
        "    #     df_features[f'pct_change_{agg}_{n}_mw'] = df_features.apply(lambda x: round(x[f'diff_{agg}_{n}_mw']/x[f'avg_{agg}_{n}_mw'], 4)\n",
        "    #                                                                        if x[f'avg_{agg}_{n}_mw'] != 0 else np.nan, axis=1)\n",
        "\n",
        "    # # Generate aggrgate features for n-point window pre and post boundary (centre windows)\n",
        "    for n in tqdm.tqdm(windows):\n",
        "        pre_break_features = df[df['position_from_breakpoint_time'].between(-n, -1)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
        "        pre_break_features.columns = [col[0] + '_' + col[1] + f'_pre_{n}' for col in  pre_break_features.columns]\n",
        "\n",
        "        post_break_features = df[df['position_from_breakpoint_time'].between(0, n-1)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
        "        post_break_features.columns = [col[0] + '_' + col[1] + f'_post_{n}' for col in  post_break_features.columns]\n",
        "\n",
        "        pre_break_features[f'value_slope_pre_{n}'] = pre_break_features.apply(lambda x: (x[f'value_last_pre_{n}'] - x[f'value_first_pre_{n}'])/x[f'value_count_pre_{n}'], axis=1)\n",
        "        post_break_features[f'value_slope_post_{n}'] = post_break_features.apply(lambda x: (x[f'value_last_post_{n}'] - x[f'value_first_post_{n}'])/x[f'value_count_post_{n}'], axis=1)\n",
        "\n",
        "        df_features = pd.concat([df_features, pre_break_features, post_break_features], axis=1)\n",
        "\n",
        "        # pre_break_features_out = df[~df['position_from_breakpoint_time'].between(-n, -1)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
        "        # post_break_features_out = df[~df['position_from_breakpoint_time'].between(0, n-1)].groupby('id').agg({'value': agg_funcs}).sort_index()\n",
        "        # pre_break_features_out.columns = [col[0] + '_' + col[1] + f'_pre_{n}_out' for col in  pre_break_features_out.columns]\n",
        "        # post_break_features_out.columns = [col[0] + '_' + col[1] + f'_post_{n}_out' for col in  post_break_features_out.columns]\n",
        "\n",
        "        # pre_break_features_out[f'value_slope_pre_{n}_out'] = pre_break_features_out.apply(lambda x: (x[f'value_last_pre_{n}_out'] - x[f'value_first_pre_{n}_out'])/x[f'value_count_pre_{n}_out'], axis=1)\n",
        "        # post_break_features_out[f'value_slope_post_{n}_out'] = post_break_features_out.apply(lambda x: (x[f'value_last_post_{n}_out'] - x[f'value_first_post_{n}_out'])/x[f'value_count_post_{n}_out'], axis=1)\n",
        "\n",
        "        # df_features = pd.concat([df_features, pre_break_features_out, post_break_features_out], axis=1)\n",
        "\n",
        "    for agg in [f[0] for f in agg_funcs]:\n",
        "        df_features[f'diff_{agg}_{n}'] = df_features[f'value_{agg}_post_{n}'] - df_features[f'value_{agg}_pre_{n}']\n",
        "        df_features[f'avg_{agg}_{n}'] =  (df_features[f'value_{agg}_post_{n}'] + df_features[f'value_{agg}_pre_{n}'])/2\n",
        "        df_features[f'ratio_{agg}_{n}'] =  df_features.apply(lambda x: round(x[f'value_{agg}_post_{n}']/x[f'value_{agg}_pre_{n}'], 4)\n",
        "                                                                           if x[f'value_{agg}_pre_{n}'] != 0 else np.nan, axis=1)\n",
        "        df_features[f'pct_change_{agg}_{n}'] = df_features.apply(lambda x: round(x[f'diff_{agg}_{n}']/x[f'avg_{agg}_{n}'], 4)\n",
        "                                                                           if x[f'avg_{agg}_{n}'] != 0 else np.nan, axis=1)\n",
        "\n",
        "    print('DONE 2: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
        "\n",
        "    # Generate statistical test features for n-point window pre and post boundary (sliding windows)\n",
        "    # for n in tqdm.tqdm(range(1, 21, 1)):\n",
        "    #     p_values_stat_tests = df[df['position_from_breakpoint_time'].between(-n, n-1)].groupby('id').apply(lambda x: extract_statistical_test_features(x, n))\n",
        "    #     p_values_stat_tests = p_values_stat_tests.droplevel(1)\n",
        "    #     df_features = pd.concat([df_features, p_values_stat_tests], axis=1)\n",
        "\n",
        "    # Generate statistical test features for n-point window pre and post boundary (centre windows)\n",
        "    for n in tqdm.tqdm(windows):\n",
        "        p_values_stat_tests = df[df['position_from_breakpoint_time'].between(-n, n-1)].groupby('id').apply(lambda x: extract_statistical_test_features(x, n))\n",
        "        p_values_stat_tests = p_values_stat_tests.droplevel(1)\n",
        "        df_features = pd.concat([df_features, p_values_stat_tests], axis=1)\n",
        "\n",
        "    print('DONE 3: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
        "\n",
        "    # # Generate frequency features for pre and post boundary signals\n",
        "    for i in (0, 1):\n",
        "        suffix = '_pre' if i == 0 else '_post'\n",
        "        freq_features = df[df['period'] == i].groupby('id').apply(lambda x: extract_frequency_features(x['value'], num_main_freqs=3))\n",
        "        freq_features = freq_features.droplevel(1).filter(regex='^freq|^power')\n",
        "        freq_features.columns = [col + suffix for col in freq_features.columns]\n",
        "        df_features = pd.concat([df_features, freq_features], axis=1)\n",
        "\n",
        "    # # Calculate percentage change between pre and post-break frequencies (n=1-3)\n",
        "    for n in range(1, 4):\n",
        "        df_features[f'diff_freq_{n}'] = df_features[f'freq_{n}_post'] - df_features[f'freq_{n}_pre']\n",
        "        df_features[f'avg_freq_{n}'] =  (df_features[f'freq_{n}_post'] + df_features[f'freq_{n}_pre'])/2\n",
        "        df_features[f'pct_change_freq_{n}'] = df_features.apply(lambda x: round(x[f'diff_freq_{n}']/x[f'avg_freq_{n}'], 4)\n",
        "                                                             if x[f'avg_freq_{n}'] != 0 else np.nan, axis=1)\n",
        "    print('DONE 4: ', datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
        "\n",
        "    feature_cols = df_features.filter(regex='^value\\_|^diff\\_|^ratio.*|^freq.*|^pval.*').columns\n",
        "    df_features = df_features[feature_cols]\n",
        "    df_features = df_features.copy()\n",
        "\n",
        "    return df_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwcDYxMh16_2"
      },
      "outputs": [],
      "source": [
        "# # Generate featrures from train data\n",
        "# X_train_features = generate_features(X_train.reset_index())\n",
        "# assert len(list(X_train_features.columns)) == len(set(X_train_features.columns))\n",
        "\n",
        "# X_train_features = pd.read_parquet('/content/data/X_train_features.parquet')\n",
        "# X_train_features = X_train_features[best_features]\n",
        "\n",
        "# Select top-k best features for training\n",
        "best_features = ['value_max_jump_pre_600', 'pval_levene_40', 'value_mean_absolute_change_post_650', 'value_ptp_pre_150', 'value_kurtosis_post_850', 'value_mean_post', 'value_min_post_600', 'value_percentile_25_pre_750', 'value_avg_fft_mag_pre_20', 'pval_ks_2samp_200', 'value_downward_steps_pre_850', 'value_max_post', 'value_kurtosis_pre', 'value_percentile_25_pre_850', 'value_percentile_25_post_450', 'pval_levene_100', 'value_mean_absolute_change_pre_250', 'value_min_post_500', 'value_percentile_25_post_250', 'value_num_turning_points_post_800', 'value_max_post_300', 'value_avg_fft_mag_pre_350', 'value_ptp_post_50', 'value_percentile_25_pre_150', 'value_kurtosis_pre_600', 'value_std_post_60', 'value_sum_fft_mag_post_750', 'value_kurtosis_pre_700', 'value_mean_pre_550', 'value_std_pre_20', 'value_avg_fft_mag_post', 'value_volatility_pre_30', 'pval_ks_2samp_90', 'value_mean_absolute_change_post_50', 'pval_ks_2samp_950', 'value_auto_1_post', 'value_auto_2_post', 'value_slope_post_350', 'value_mean_absolute_change_pre_700', 'value_std_pre_700', 'value_kurtosis_post_600', 'value_min_pre_70', 'value_mean_absolute_change_pre_60', 'value_percentile_25_pre_80', 'value_avg_fft_mag_post_650', 'pval_ks_2samp_30', 'value_mean_pre_450', 'value_max_pre_300', 'value_auto_2_pre_900', 'value_percentile_25_post_90', 'value_percentile_25_pre', 'value_min_pre_950', 'value_mean_absolute_change_post_90', 'value_max_post_40', 'value_num_turning_points_pre_10', 'value_upward_steps_pre_950', 'value_ptp_post_250', 'value_avg_fft_mag_post_500', 'value_ptp_pre_250', 'value_std_post_10', 'value_ptp_post_600', 'value_downward_steps_post_200', 'value_sum_fft_mag_post_400', 'value_downward_steps_post', 'value_mean_post_500', 'value_auto_1_post_450', 'value_max_jump_pre_70', 'value_num_turning_points_pre_200', 'pval_levene_400', 'value_mean_post_650', 'value_percentile_25_post_150', 'value_skew_post_800', 'value_std_post_20', 'value_ptp_post_5', 'diff_max_jump', 'pval_ks_2samp_60', 'value_mean_absolute_change_pre_450', 'value_auto_3_pre_350', 'ratio_mean_absolute_change', 'diff_mean', 'value_ptp_pre_500', 'pval_levene_30', 'value_num_turning_points_post_80', 'value_volatility_post_90', 'value_ptp_pre_700', 'value_upward_steps_post_90', 'value_upward_steps_pre_250', 'value_percentile_25_pre_250', 'value_volatility_post_300', 'value_num_turning_points_post_50', 'value_max_jump_pre_10', 'value_volatility_pre_20', 'value_dominant_freq_ind_post_800', 'value_ptp_pre_550', 'value_upward_steps_pre_70', 'value_std_pre_500', 'ratio_std_1000', 'pval_ks_2samp_150', 'pval_ks_2samp_500', 'value_ptp_pre_950', 'value_percentile_75_post_450', 'value_std_post_80', 'pval_ttest_ind_950', 'value_volatility_pre_700', 'value_ptp_pre_650', 'value_upward_steps_post', 'pval_ks_2samp_700', 'value_upward_steps_pre_60', 'value_auto_1_pre_800', 'ratio_mean_absolute_change_1000', 'value_skew_post_400', 'value_auto_3_post_550', 'value_mean_pre_350', 'value_num_turning_points_pre_250', 'pval_ks_2samp_5', 'pval_levene_80', 'value_auto_1_pre_700', 'value_downward_steps_post_500', 'value_trend_post_350', 'ratio_num_turning_points', 'value_volatility_pre_750', 'value_median_pre_250', 'value_trend_post_80', 'value_skew_post', 'value_max_pre_80', 'value_ptp_post_750', 'value_mean_absolute_change_post_60', 'value_avg_fft_mag_pre', 'value_mean_pre_800', 'value_median_pre_150', 'value_percentile_75_pre_600', 'ratio_median', 'pval_ks_2samp_20', 'value_skew_pre_40', 'diff_percentile_25', 'value_trend_pre_150', 'value_kurtosis_post_150', 'value_auto_2_post_550', 'pval_levene_750', 'value_upward_steps_pre_350', 'value_max_jump_pre', 'value_mean_absolute_change_pre_650', 'value_percentile_25_post_500', 'pval_ks_2samp_550', 'value_auto_2_post_300', 'value_max_jump_post_250', 'diff_num_turning_points_1000', 'value_ptp_pre', 'value_mean_absolute_change_pre_20', 'value_dominant_freq_ind_post_950']\n",
        "\n",
        "# Select best params for model training\n",
        "best_params = {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY7pYEz2hjCb"
      },
      "outputs": [],
      "source": [
        "# def validate(X_train_features: pd.DataFrame, y_train: pd.Series):\n",
        "#     # Train classifier model to predict 'period'\n",
        "#     model = XGBClassifier(n_estimators=500, objective='binary:logistic', max_depth=6, n_jobs=-1, random_state=15)\n",
        "#     print('RUNNING CROSS VALIDATION :')\n",
        "#     roc_auc_scores = cross_val_score(model, X_train_features, y_train.values, cv=5, scoring='roc_auc', verbose=1)\n",
        "#     return roc_auc_scores\n",
        "\n",
        "# roc_auc_scores = validate(X_train_features, y_train)\n",
        "# print(roc_auc_scores)\n",
        "# print(roc_auc_scores.mean(), roc_auc_scores.std())\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "# X_train_features.to_parquet('X_train_features.parquet')\n",
        "# files.download('X_train_features.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQwWDC6M-0fT"
      },
      "outputs": [],
      "source": [
        "def train(X_train: pd.DataFrame, y_train: pd.Series, model_directory_path: str):\n",
        "    # Generate featrures from train data\n",
        "    X_train_features = generate_features(X_train.reset_index())\n",
        "    X_train_features = X_train_features[best_features]\n",
        "\n",
        "    # Train classifier model to predict 'period'\n",
        "    model = XGBClassifier(**best_params, objective='binary:logistic', n_jobs=-1, random_state=15)\n",
        "    print('TRAINING MODEL :')\n",
        "    model.fit(X_train_features, y_train.values)\n",
        "\n",
        "    # Get prediction from trained model\n",
        "    # y_train_prediction = model.predict(X_train_features)\n",
        "    # train_auc = sklearn.metrics.roc_auc_score(y_train['structural_breakpoint'], y_train_prediction)\n",
        "\n",
        "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93bdpiA_CGpf"
      },
      "outputs": [],
      "source": [
        "def feature_importance(X_train: pd.DataFrame, y_train: pd.Series):\n",
        "    model = XGBClassifier(n_estimators=500, objective='binary:logistic', max_depth=6, n_jobs=-1, random_state=15)\n",
        "\n",
        "    # Put into a DataFrame for easy sorting\n",
        "    feat_imp = pd.DataFrame({\n",
        "        \"feature\": X_train.columns,\n",
        "        \"importance\": model.feature_importances_})\n",
        "\n",
        "    # Sort by importance (descending)\n",
        "    feat_imp = feat_imp.sort_values(by=\"importance\", ascending=False)\n",
        "    return feat_imp\n",
        "\n",
        "# feat_imp = feature_importance(X_train, y_train)\n",
        "# feat_imp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-jboJH-0fU"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZlEQkSK_H3H"
      },
      "outputs": [],
      "source": [
        "# Infer structural break based on trained model predictions\n",
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "    yield  # Mark as ready\n",
        "    best_features = ['value_max_jump_pre_600', 'pval_levene_40', 'value_mean_absolute_change_post_650', 'value_ptp_pre_150', 'value_kurtosis_post_850', 'value_mean_post', 'value_min_post_600', 'value_percentile_25_pre_750', 'value_avg_fft_mag_pre_20', 'pval_ks_2samp_200', 'value_downward_steps_pre_850', 'value_max_post', 'value_kurtosis_pre', 'value_percentile_25_pre_850', 'value_percentile_25_post_450', 'pval_levene_100', 'value_mean_absolute_change_pre_250', 'value_min_post_500', 'value_percentile_25_post_250', 'value_num_turning_points_post_800', 'value_max_post_300', 'value_avg_fft_mag_pre_350', 'value_ptp_post_50', 'value_percentile_25_pre_150', 'value_kurtosis_pre_600', 'value_std_post_60', 'value_sum_fft_mag_post_750', 'value_kurtosis_pre_700', 'value_mean_pre_550', 'value_std_pre_20', 'value_avg_fft_mag_post', 'value_volatility_pre_30', 'pval_ks_2samp_90', 'value_mean_absolute_change_post_50', 'pval_ks_2samp_950', 'value_auto_1_post', 'value_auto_2_post', 'value_slope_post_350', 'value_mean_absolute_change_pre_700', 'value_std_pre_700', 'value_kurtosis_post_600', 'value_min_pre_70', 'value_mean_absolute_change_pre_60', 'value_percentile_25_pre_80', 'value_avg_fft_mag_post_650', 'pval_ks_2samp_30', 'value_mean_pre_450', 'value_max_pre_300', 'value_auto_2_pre_900', 'value_percentile_25_post_90', 'value_percentile_25_pre', 'value_min_pre_950', 'value_mean_absolute_change_post_90', 'value_max_post_40', 'value_num_turning_points_pre_10', 'value_upward_steps_pre_950', 'value_ptp_post_250', 'value_avg_fft_mag_post_500', 'value_ptp_pre_250', 'value_std_post_10', 'value_ptp_post_600', 'value_downward_steps_post_200', 'value_sum_fft_mag_post_400', 'value_downward_steps_post', 'value_mean_post_500', 'value_auto_1_post_450', 'value_max_jump_pre_70', 'value_num_turning_points_pre_200', 'pval_levene_400', 'value_mean_post_650', 'value_percentile_25_post_150', 'value_skew_post_800', 'value_std_post_20', 'value_ptp_post_5', 'diff_max_jump', 'pval_ks_2samp_60', 'value_mean_absolute_change_pre_450', 'value_auto_3_pre_350', 'ratio_mean_absolute_change', 'diff_mean', 'value_ptp_pre_500', 'pval_levene_30', 'value_num_turning_points_post_80', 'value_volatility_post_90', 'value_ptp_pre_700', 'value_upward_steps_post_90', 'value_upward_steps_pre_250', 'value_percentile_25_pre_250', 'value_volatility_post_300', 'value_num_turning_points_post_50', 'value_max_jump_pre_10', 'value_volatility_pre_20', 'value_dominant_freq_ind_post_800', 'value_ptp_pre_550', 'value_upward_steps_pre_70', 'value_std_pre_500', 'ratio_std_1000', 'pval_ks_2samp_150', 'pval_ks_2samp_500', 'value_ptp_pre_950', 'value_percentile_75_post_450', 'value_std_post_80', 'pval_ttest_ind_950', 'value_volatility_pre_700', 'value_ptp_pre_650', 'value_upward_steps_post', 'pval_ks_2samp_700', 'value_upward_steps_pre_60', 'value_auto_1_pre_800', 'ratio_mean_absolute_change_1000', 'value_skew_post_400', 'value_auto_3_post_550', 'value_mean_pre_350', 'value_num_turning_points_pre_250', 'pval_ks_2samp_5', 'pval_levene_80', 'value_auto_1_pre_700', 'value_downward_steps_post_500', 'value_trend_post_350', 'ratio_num_turning_points', 'value_volatility_pre_750', 'value_median_pre_250', 'value_trend_post_80', 'value_skew_post', 'value_max_pre_80', 'value_ptp_post_750', 'value_mean_absolute_change_post_60', 'value_avg_fft_mag_pre', 'value_mean_pre_800', 'value_median_pre_150', 'value_percentile_75_pre_600', 'ratio_median', 'pval_ks_2samp_20', 'value_skew_pre_40', 'diff_percentile_25', 'value_trend_pre_150', 'value_kurtosis_post_150', 'value_auto_2_post_550', 'pval_levene_750', 'value_upward_steps_pre_350', 'value_max_jump_pre', 'value_mean_absolute_change_pre_650', 'value_percentile_25_post_500', 'pval_ks_2samp_550', 'value_auto_2_post_300', 'value_max_jump_post_250', 'diff_num_turning_points_1000', 'value_ptp_pre', 'value_mean_absolute_change_pre_20', 'value_dominant_freq_ind_post_950']\n",
        "\n",
        "    # X_test can only be iterated once.\n",
        "    # Before getting the next dataset, you must predict the current one.\n",
        "    for dataset in X_test:\n",
        "        def predict(df_features: pd.DataFrame):\n",
        "            prob = model.predict_proba(df_features)[:, 1]\n",
        "            return prob\n",
        "\n",
        "        dataset = dataset.reset_index()\n",
        "\n",
        "        # Generate features from values before and after breakpoint\n",
        "        X_test_features = generate_features(dataset)\n",
        "        X_test_features = X_test_features[best_features]\n",
        "\n",
        "        prediction = predict(X_test_features)\n",
        "        yield prediction  # Send the prediction for the current dataset\n",
        "\n",
        "        # Note: This baseline approach uses a t-test to compare the distributions\n",
        "        # before and after the boundary point. A smaller p-value (larger negative number)\n",
        "        # suggests stronger evidence that the distributions are different,\n",
        "        # indicating a potential structural break."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkMI-IOd4Qez"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDZeP-4--0fU"
      },
      "outputs": [],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "## Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly5q68sA-0fU"
      },
      "outputs": [],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCrjpzv-0fU"
      },
      "outputs": [],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "sklearn.metrics.roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AE1i3pR-0fV"
      },
      "source": [
        "# Submit your Notebook\n",
        "\n",
        "To submit your work, you must:\n",
        "1. Download your Notebook from Colab\n",
        "2. Upload it to the platform\n",
        "3. Create a run to validate it\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Download and Submit Notebook](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/download-and-submit-notebook.gif)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}