{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyarrow==6.0.1\n",
    "# pip show fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy, normaltest, jarque_bera\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from local parquet file\n",
    "X_train = pd.read_parquet(\"data/X_train.parquet\")\n",
    "X_train_features = pd.read_parquet(\"data/X_train_features.parquet\")\n",
    "y_train = pd.read_parquet(\"data/y_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = X_train_features.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_features[X_train_features['ratio_median']\\\n",
    "#          .isnull()][['value_median_pre', 'value_median_post', 'ratio_median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with medians\n",
    "for col in missing_values[missing_values > 0].index:\n",
    "    X_train_features[col] = X_train_features[col].fillna(X_train_features[col].median())     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_values = scaler.fit_transform(X_train_features)\n",
    "y_values = y_train.replace({False:0, True:1}).values\n",
    "\n",
    "# Train-test split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_values, y_values, test_size=0.2, stratify=y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.isnan(X_values).any(), \"NaNs in input features\"\n",
    "assert not np.isnan(y_values).any(), \"NaNs in labels\"\n",
    "assert not np.any(np.isinf(X_values)), \"Infs in input features\"\n",
    "assert not np.any(np.isinf(y_values)), \"Infs in labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        # layers.Dropout(0.4),\n",
    "        # layers.Dense(64),\n",
    "        # layers.LeakyReLU(),\n",
    "        layers.Dense(1, activation='sigmoid')  # Output probability\n",
    "    ])\n",
    "    opt = optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.0)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_auc', # Metric to monitor (e.g., 'val_loss', 'val_auc')\n",
    "    patience=10, # Number of epochs to wait before stopping\n",
    "    restore_best_weights=True, # Roll back to best model\n",
    "    mode='max', # 'min' for loss, 'max' for AUC\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5)\n",
    "\n",
    "# Build and train\n",
    "model = build_model(X_tr.shape[1])\n",
    "history = model.fit(X_tr, y_tr,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    callbacks=[early_stop, lr_schedule],\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_val).flatten()\n",
    "auc_score = roc_auc_score(y_val, y_pred)\n",
    "print(f\"Validation ROC-AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train & evaluation metrics\n",
    "def plot_training_metrics(history):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # 🔻 Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Binary Crossentropy Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 🔺 AUC Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['auc'], label='Training AUC', color='green')\n",
    "    plt.plot(epochs, history.history['val_auc'], label='Validation AUC', color='red')\n",
    "    plt.title('AUC over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ROC-AUC Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX AUC ~ 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
    "    scheduler_fn=StepLR,\n",
    "    mask_type='entmax'  # 'sparsemax' also available\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train=X_tr, y_train=y_tr.flatten(),\n",
    "    eval_set=[(X_val, y_val.flatten())],\n",
    "    eval_name=['val'],\n",
    "    eval_metric=['auc'],\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX AUC ~ 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = X_train.loc[0:10000]\n",
    "y_train_sample = y_train.loc[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = X_train_sample.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(segment, windows=[5, 10, 15, 20,\n",
    "                                       30, 40, 50, 60, 70, 80, 90,\n",
    "                                       100, 200, 300, 400, 500,\n",
    "                                       600, 700, 800, 900, 1000]):\n",
    "    features = []\n",
    "    for w in windows:\n",
    "        if len(segment) < w:\n",
    "            rolling = pd.Series(segment).rolling(window=len(segment))\n",
    "        rolling = pd.Series(segment).rolling(window=w)\n",
    "        features.extend([\n",
    "            rolling.mean().dropna().mean(),\n",
    "            rolling.std().dropna().mean(),\n",
    "            rolling.skew().dropna().mean(),\n",
    "            rolling.kurt().dropna().mean(),\n",
    "            np.mean(np.diff(segment)),  # trend\n",
    "            np.std(np.diff(segment)),   # volatility\n",
    "            np.max(segment) - np.min(segment),  # range\n",
    "        ])\n",
    "        \n",
    "        features.extend([\n",
    "            # Central tendency & dispersion\n",
    "            np.median(segment),\n",
    "            np.percentile(segment, 25),\n",
    "            np.percentile(segment, 75),\n",
    "            np.var(segment),\n",
    "            np.ptp(segment),  # peak-to-peak range\n",
    "\n",
    "            # Distribution shape\n",
    "            # entropy(np.histogram(segment, bins=20, density=True)[0]),  # entropy\n",
    "            # normaltest(segment)[0],  # deviation from normality\n",
    "            # jarque_bera(segment)[0],  # skew/kurtosis test\n",
    "\n",
    "            # Autocorrelation & stationarity\n",
    "            pd.Series(segment).autocorr(lag=1),\n",
    "            pd.Series(segment).autocorr(lag=5),\n",
    "            # adfuller(segment)[0],  # ADF test statistic (stationarity)\n",
    "\n",
    "            # Frequency domain\n",
    "            np.mean(np.abs(np.fft.fft(segment))),  # average FFT magnitude\n",
    "            np.argmax(np.abs(np.fft.fft(segment))),  # dominant frequency index\n",
    "            np.sum(np.abs(np.fft.fft(segment))[:10]),  # low-frequency energy\n",
    "\n",
    "            # Change dynamics\n",
    "            np.mean(np.abs(np.diff(segment))),  # mean absolute change\n",
    "            np.max(np.abs(np.diff(segment))),   # max jump\n",
    "            np.std(np.diff(segment)),           # change volatility\n",
    "\n",
    "            # Complexity\n",
    "            np.sum(np.diff(np.sign(np.diff(segment))) != 0),  # number of turning points\n",
    "            np.sum(np.diff(segment) > 0),                     # upward steps\n",
    "            np.sum(np.diff(segment) < 0),                     # downward steps\n",
    "        ])\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data):\n",
    "    X0, X1, y = [], [], []\n",
    "    for item in tqdm.tqdm(data):\n",
    "        f0 = extract_features(item['period_0'])\n",
    "        f1 = extract_features(item['period_1'])\n",
    "        if f0.shape[0] != f1.shape[0]:  # skip if unequal\n",
    "            continue\n",
    "        X0.append(f0)\n",
    "        X1.append(f1)\n",
    "        y.append(item['label'])\n",
    "\n",
    "    X0, X1, y = np.array(X0), np.array(X1), np.array(y)\n",
    "    scaler = StandardScaler()\n",
    "    X0 = scaler.fit_transform(X0)\n",
    "    X1 = scaler.transform(X1)\n",
    "    \n",
    "    return X0, X1, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_model(input_dim):\n",
    "    def base_network():\n",
    "        inp = Input(shape=(input_dim,))\n",
    "        x = layers.Dense(256)(inp)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        return models.Model(inp, x)\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    \n",
    "    base = base_network()\n",
    "    feat_a = base(input_a)\n",
    "    feat_b = base(input_b)\n",
    "    \n",
    "    # Compare embeddings\n",
    "    merged = layers.Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([feat_a, feat_b])\n",
    "    x = layers.Dense(16, activation='relu')(merged)\n",
    "    x = layers.Dense(8, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[input_a, input_b], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.isnan(X0).any(), \"NaNs in input features\"\n",
    "assert not np.isnan(X1).any(), \"NaNs in labels\"\n",
    "assert not np.any(np.isinf(X0)), \"Infs in input features\"\n",
    "assert not np.any(np.isinf(X1)), \"Infs in labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual dataset\n",
    "data = []\n",
    "for i in tqdm.tqdm(range(10001)):\n",
    "    df = X_train_sample.loc[i]\n",
    "    label = y_train_sample.loc[i]\n",
    "    data.append({'id': i,\n",
    "                 'period_0': df[df['period'] == 0]['value'],\n",
    "                 'period_1': df[df['period'] == 1]['value'],\n",
    "                 'label': label['structural_breakpoint']})\n",
    "print('GENERATE DATASET : DONE')\n",
    "print('RUNNING DATA PREPARATION')\n",
    "X0, X1, y = prepare_dataset(data)\n",
    "\n",
    "print('PREPARE DATASET : DONE')\n",
    "X0_train, X0_val, X1_train, X1_val, y_train, y_val = train_test_split(X0, X1, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Fill nans in prepared dataset\n",
    "column_medians = np.nanmedian(X1_train, axis=0)\n",
    "column_medians = np.nan_to_num(column_medians, nan=0)\n",
    "nan_indices = np.where(np.isnan(X1_train))\n",
    "X1_train[nan_indices] = np.take(column_medians, nan_indices[1])\n",
    "\n",
    "column_medians = np.nanmedian(X1_val, axis=0)\n",
    "column_medians = np.nan_to_num(column_medians, nan=0)\n",
    "nan_indices = np.where(np.isnan(X1_val))\n",
    "X1_val[nan_indices] = np.take(column_medians, nan_indices[1])\n",
    "\n",
    "model = build_siamese_model(X0.shape[1])\n",
    "history = model.fit([X0_train, X1_train], y_train, validation_data=([X0_val, X1_val], y_val),\n",
    "                    epochs=100, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X0_val, X1_val]).flatten()\n",
    "auc_score = roc_auc_score(y_val, y_pred)\n",
    "print(f\"Validation ROC-AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train & evaluation metrics\n",
    "def plot_training_metrics(history):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # 🔻 Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Binary Crossentropy Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 🔺 AUC Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['auc'], label='Training AUC', color='green')\n",
    "    plt.plot(epochs, history.history['val_auc'], label='Validation AUC', color='red')\n",
    "    plt.title('AUC over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ROC-AUC Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_siamese(X0, X1, y, k=5, epochs=20, batch_size=128):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X0, y)):\n",
    "        print(f\"\\n🔁 Fold {fold + 1}/{k}\")\n",
    "\n",
    "        X0_train, X0_val = X0[train_idx], X0[val_idx]\n",
    "        X1_train, X1_val = X1[train_idx], X1[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = build_siamese_model(X0.shape[1])\n",
    "        model.fit([X0_train, X1_train], y_train,\n",
    "                  validation_data=([X0_val, X1_val], y_val),\n",
    "                  epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        y_pred = model.predict([X0_val, X1_val]).flatten()\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        print(f\"✅ Fold {fold + 1} ROC-AUC: {auc:.4f}\")\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    print(f\"\\n📊 Mean ROC-AUC over {k} folds: {np.mean(auc_scores):.4f}\")\n",
    "    return auc_scores\n",
    "\n",
    "auc_scores = cross_validate_siamese(X0, X1, y, k=5, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Feature Extraction with 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor(input_length):\n",
    "    inp = Input(shape=(input_length, 1))  # univariate time series\n",
    "    x = layers.LayerNormalization(axis=-1)(inp)\n",
    "    x = layers.Masking(mask_value=0., input_shape=(input_length, 1))(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=64, kernel_size=7, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)  # reduces to fixed-size vector\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    return models.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor(input_length):\n",
    "    inp = Input(shape=(input_length, 1))\n",
    "    x = layers.Masking(mask_value=0., input_shape=(input_length, 1))(inp)\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv1D(64, kernel_size=7, dilation_rate=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv1D(128, kernel_size=5, dilation_rate=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x1 = layers.Conv1D(256, kernel_size=3, dilation_rate=4, padding='same')(x)\n",
    "    x1 = layers.BatchNormalization()(x1)\n",
    "    x1 = layers.ReLU()(x1)\n",
    "\n",
    "    # Residual connection\n",
    "    shortcut = layers.Conv1D(256, kernel_size=1, dilation_rate=1, padding='same')(x)\n",
    "    shortcut = layers.BatchNormalization()(shortcut)\n",
    "    x = layers.Add()([x1, shortcut])\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    return models.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_raw_model(input_length_a, input_length_b):\n",
    "    feature_extractor_a = build_feature_extractor(input_length_a)\n",
    "    feature_extractor_b = build_feature_extractor(input_length_b)\n",
    "\n",
    "    input_a = Input(shape=(input_length_a, 1))  # period_0\n",
    "    input_b = Input(shape=(input_length_b, 1))  # period_1\n",
    "\n",
    "    feat_a = feature_extractor_a(input_a)\n",
    "    feat_b = feature_extractor_b(input_b)\n",
    "\n",
    "    # Compare embeddings\n",
    "    merged = layers.Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([feat_a, feat_b])\n",
    "    x = layers.Dense(16, activation='relu')(merged)\n",
    "    x = layers.Dense(8, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = models.Model(inputs=[input_a, input_b], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_data(data, segment_length_a='max', segment_length_b='max'):\n",
    "    X0, X1, y = [], [], []\n",
    "    for item in tqdm.tqdm(data):\n",
    "        if isinstance(segment_length_a, int) and segment_length_a <= len(item['period_0']):\n",
    "            p0 = np.array(item['period_0'][(-segment_length_a):])\n",
    "        else:\n",
    "            p0 = np.array(item['period_0'])[:]\n",
    "        if isinstance(segment_length_b, int) and segment_length_b <= len(item['period_1']):\n",
    "            p1 = np.array(item['period_1'][:segment_length_b])\n",
    "        else:\n",
    "            p1 = np.array(item['period_1'])[:]\n",
    "       \n",
    "        X0.append(p0.reshape(-1, 1))\n",
    "        X1.append(p1.reshape(-1, 1))\n",
    "        y.append(item['label'])\n",
    "        \n",
    "    # Pad sequences to a max length\n",
    "    X0 = pad_sequences(X0, maxlen=max_len_a, padding='pre', value=0)\n",
    "    X1 = pad_sequences(X1, maxlen=max_len_b, padding='post', value=0)\n",
    "\n",
    "    return np.array(X0), np.array(X1), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from local parquet file\n",
    "# X_train = pd.read_parquet(\"data/X_train.parquet\")\n",
    "# y_train = pd.read_parquet(\"data/y_train.parquet\")\n",
    "\n",
    "# Print max series lengths for period 0/1\n",
    "# series_len = X_train.groupby(['id', 'period'])['value'].count().to_frame()\n",
    "# idx = pd.IndexSlice\n",
    "# print(series_len.loc[idx[:, 0], :]['value'].max(), series_len.loc[idx[:, 1], :]['value'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "max_len_a = 2500\n",
    "max_len_b = 1000\n",
    "\n",
    "for i in tqdm.tqdm(range(10001)):\n",
    "    df = X_train.loc[i]\n",
    "    label = y_train.loc[i]\n",
    "    data.append({'id': i,\n",
    "                 'period_0': df[df['period'] == 0]['value'],\n",
    "                 'period_1': df[df['period'] == 1]['value'],\n",
    "                 'label': label['structural_breakpoint']})\n",
    "    \n",
    "print('GENERATE DATASET : DONE')\n",
    "print('RUNNING DATA PREPARATION')\n",
    "X0_raw, X1_raw, y_raw = prepare_raw_data(data, segment_length_a='max', segment_length_b='max')\n",
    "\n",
    "print('PREPARE DATASET : DONE')\n",
    "X0_tr, X0_val, X1_tr, X1_val, y_tr, y_val = train_test_split(X0_raw, X1_raw, y_raw, test_size=0.2, stratify=y_raw)\n",
    "\n",
    "model = build_siamese_raw_model(input_length_a=max_len_a, input_length_b=max_len_b)\n",
    "history = model.fit([X0_tr, X1_tr], y_tr, validation_data=([X0_val, X1_val], y_val),\n",
    "                     epochs=100, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X0_val, X1_val]).flatten()\n",
    "auc_score = roc_auc_score(y_val, y_pred)\n",
    "print(f\"Validation ROC-AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train & evaluation metrics\n",
    "def plot_training_metrics(history):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # 🔻 Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Binary Crossentropy Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 🔺 AUC Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['auc'], label='Training AUC', color='green')\n",
    "    plt.plot(epochs, history.history['val_auc'], label='Validation AUC', color='red')\n",
    "    plt.title('AUC over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ROC-AUC Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_siamese(X0, X1, y, k=5, epochs=50, batch_size=512):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X0, y)):\n",
    "        print(f\"\\n🔁 Fold {fold + 1}/{k}\")\n",
    "\n",
    "        X0_train, X0_val = X0[train_idx], X0[val_idx]\n",
    "        X1_train, X1_val = X1[train_idx], X1[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = build_siamese_raw_model(1000, 100)\n",
    "        model.fit([X0_train, X1_train], y_train,\n",
    "                  validation_data=([X0_val, X1_val], y_val),\n",
    "                  epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        y_pred = model.predict([X0_val, X1_val]).flatten()\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        print(f\"✅ Fold {fold + 1} ROC-AUC: {auc:.4f}\")\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    print(f\"\\n📊 Mean ROC-AUC over {k} folds: {np.mean(auc_scores):.4f}\")\n",
    "    return auc_scores\n",
    "\n",
    "auc_scores = cross_validate_siamese(X0_raw, X1_raw, y_raw, k=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
